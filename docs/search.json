[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dise√±os experimentales y cuasiexperimentales",
    "section": "",
    "text": "Este libro surge con la intenci√≥n de acompa√±ar a los estudiantes de la materia Dise√±o experimentales y cuasiexperimentales correspondiente a la carrera de Licenciatura en Ciencias del Comportamiento de la Universidad de San Andr√©s (Victoria, Argentina). Sin embargo, este libro no contiene todo los materiales de la materia, sino m√°s bien es una gu√≠a que nos permite comprender conceptos b√°sicos e identificar lo importante en cada tema. De forma complementaria, en cada cap√≠tulo se brindar√° una lista de bibliograf√≠a recomendada.\n\n\nEl objetivo de este libro es acercar a los lectores y lectoras los conceptos b√°sicos del dise√±o experimental y cuasiexperimental para que de esta manera sean capaces de dise√±ar sus propios experimentos para expresar y verificar efectivamente hip√≥tesis cient√≠ficas. Asimismo se busca generar intuiciones que les permitan evaluar dise√±os experimentales con los que se puedan encontrar tanto en la literatura cient√≠fica como en cualquier dato obtenido experimentalmente que se les presente al momento de tomar una decisi√≥n.\nSe espera que los lectores y lectoras sean capaces de comunicar los dise√±os experimentales, sus resultados y las implicancias de los mismos de manera clara, concisa y ‚Äúapta para todo p√∫blico‚Äù. As√≠mismo, un concepto transversal es que no siempre es posible implementar un dise√±o experimental ‚Äúideal‚Äù (si este existiera) y que tomar decisiones informadas sobre los mismos (modelos estad√≠sticos, m√©tricas, dise√±os, etc.) es una parte importante de nuestra labor como generadores de evidencia o tomadores de decisi√≥n basada en evidencia.\n\n\n\n\n\n\nSIGA SIGA‚Ä¶\n\n\n\nSi bien los ejemplos que se presentan en el libro est√°n orientados a las ciencias del comportamiento, su contenido puede ser adaptado a cualquier ciencia experimental, desde la biolog√≠a hasta la econom√≠a.\n\n\n\n\n\nEn este libro no vas a encontrar grandes desarrollos te√≥ricos ni ejemplos complejos. Es simplemente una gu√≠a de lectura que tiene como objetivo ser la puerta de entrada para algunos conceptos clave del dise√±o experimental y la inferencia causal. Para esto vamos a chapotear en R y tidyverse, mojar las patas en la estad√≠stica, aguantar un poco la respiraci√≥n en inferencia y potential outcomes, bucear en dise√±os aleatorizados (experimentos) para finalmente sumergirnos (con la idea de volver a salir a flote) en las profundas aguas del dise√±o cuasiexperimental y la inferencia causal.\n\n\n\nEste libro no es un libro de inferencia estad√≠stica y no vamos a hablar de tests estad√≠sticos sino en el contexto de una hip√≥tesis cient√≠fica y de un dise√±o experimental en particular. Es decir, este libro NO es un manual de estad√≠stica. Existen cientos de libros que desarrollan en detalle esos contenidos y para nada este libro pretende cubrir esos contenidos.\nEste libro tampoco es un manual de programaci√≥n ni de an√°lisis de datos en R. Si bien en la primera secci√≥n del libro haremos una presentaci√≥n de algunas de las funcionalidades de la colecci√≥n de paquetes para an√°lisis de datos que es el tidyverse, no vamos a repasar ninguna de las bases de R ni de Rstudio. Existen infinidad de recursos invre√≠bles para aprender esto y sentimos que no hay necesida de, a eso, sumarle uno mediocre.\n\n\n\nPara sacarle el jugo a los contenidos de este libro hace falta tener conocimientos b√°sicos de probabilidad y estad√≠stica as√≠ como estar familiarizado con la programaci√≥n en R. Ninguno de estos son obst√°culos hoy en d√≠a ya que hay cientos de fuentes (libros, cursos, etc.) a las que el lector puede consultar previo o durante la lectura de este libro.\n\n\n\n\n\n\nDON‚ÄôT PANIC\n\n\n\nEl libro comienza con un breve repaso de conceptos b√°sicos de probabilidad y estad√≠stica y presenta los comandos b√°sicos para correr c√≥digos en R.\n\n\n\n\n\n\n\n\nAnte cualquier duda pueden contactarme v√≠a e-mail a ispiousas@udesa.edu.ar.\n\n\n\nEste sitio web es y siempre ser√° gratuito, licencia bajo [CC BY-NC-ND 3.0 License]{https://creativecommons.org/licenses/by-nc-nd/3.0/us/}."
  },
  {
    "objectID": "intro_R.html",
    "href": "intro_R.html",
    "title": "1¬† R y el tidyverse",
    "section": "",
    "text": "En la gran mayor√≠a de las ejemplos y ejercicios de este libro vamos a usar una computadora (te quiero mucho Skynet ‚ô•Ô∏è). Con ella nos vamos a comunicar utilizando un lenguaje de programaci√≥n muy popular en el campo de la estad√≠stica: R(R Core Team 2023). Por eso mi recomendaci√≥n es que lo que primero ten√©s que hacer es instalar R y RStudio. RStudio es una interfaz muy popular utilizada para, mayormente, programar en R. En el recuadro siguiente van a encontrar informaci√≥n de c√≥mo instalar ambas cosas."
  },
  {
    "objectID": "intro_R.html#el-tidyverse",
    "href": "intro_R.html#el-tidyverse",
    "title": "1¬† R y el tidyverse",
    "section": "1.1 El tidyverse",
    "text": "1.1 El tidyverse\n\n1.1.1 Tidy data\nLo primero que tenemos que pensar cuando trabajamos con el tidyverse es que nuestros datos est√©n en formato tidy. ¬øQu√© significa esto? Cuando un dataset est√° en formato tidy, cada columna corresponde a una variable y cada fila a una √∫nica observaci√≥n2. Veamos un ejemplo. Tenemos tres sujetos a los cuales les medimos el tiempo de respuesta en una tarea. Cada sujeto realiza dos repeticiones de esta medici√≥n, el trial 1 y el trial 2. En la tabla Table¬†1.1 podemos ver las dos formas de organizar esta informaci√≥n.2¬†El caso contrario ser√≠a en el que una fila contiene varios mediciones para distintos niveles de una variable. Este formato se conoce como wide.\n\n\nTable¬†1.1: Ejemplo de tablas tidy y wide.\n\n\n\n\n(a) Tidy \n \n  \n    sujeto \n    trial \n    tiempo_respuesta \n  \n \n\n  \n    Jerry \n    1 \n    0.0807501 \n  \n  \n    Jerry \n    2 \n    0.8343330 \n  \n  \n    Elaine \n    1 \n    0.6007609 \n  \n  \n    Elaine \n    2 \n    0.1572084 \n  \n  \n    George \n    1 \n    0.0073994 \n  \n  \n    George \n    2 \n    0.4663935 \n  \n\n\n\n\n\n\n(b) Wide \n \n  \n    sujeto \n    trial_1 \n    trial_2 \n  \n \n\n  \n    Jerry \n    0.4977774 \n    0.7725215 \n  \n  \n    Elaine \n    0.2897672 \n    0.8746007 \n  \n  \n    George \n    0.7328820 \n    0.1749406 \n  \n\n\n\n\n\n\nA lo largo de este cap√≠tulo iremos viendo los beneficios de almacenar los datos en formato tidy. Por supuesto que estas ventajas tienen su precio, principalemente que las bases de datos crecen mucho en tama√±o si tenemos muchas medidas repetidas con distintos valores de las variables.\n\n\n1.1.2 Introducci√≥n al Tidyverse\nComo contamos m√°s arriba, el tidyverse es una colecci√≥n cerca de 25 paquetes, todos relacionados con la carga, manejo, modificaci√≥n y visualizaci√≥n de datos. La idea de este libro no es profundizar en todas sus capacidades pero consideramos importante presentar algunas de las funciones que m√°s vamos a utilizar a lo largo del libro. Estas son funciones para leer datos del paquete {readr}, los verbos de {dplyr} para manipularlos, las funciones de {tidyR} para acomodarlos y el poderos√≠simo {ggplot2} para visualizarlos.\n\n1.1.2.1 Cargando datos con readr\nUna de las cosas que vamos a hacer m√°s a menudo en este libro es cargar alg√∫n dataset. Para esto vamos a usar varias de las funcionalidades del paquete {readr}.\nEl caso m√°s simple al que nos vamos a enfrentar es la carga de una base de datos organizada en columnas y separadas por comas en un archivo de extensi√≥n .csv. En este caso lo que tenemos que hacer es bastante simple, usar la funci√≥n read_csv() como a continuaci√≥n:\n\ndata <- read_csv(\"../data/summer.csv\")\n#> Rows: 31165 Columns: 9\n#> ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Delimiter: \",\"\n#> chr (8): City, Sport, Discipline, Athlete, Country, Gender, Event, Medal\n#> dbl (1): Year\n#> \n#> ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n#> ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nsummary(data)\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\nLos datos adentro de summer.csv son los ganadores de medallas en los juegos ol√≠mpicos de invierno. El formato en el que read_csv() almacena los datos se llama tibble y es el formato por excelencia del tidyverse. De momento lo √∫nico que nos importa es que es un formato que almacena los casos en filas y las variables en columnas (cada variable tiene un formato). Para m√°s informaci√≥n sobre las cualidades de este formato, les recomiendo revisar la documentaci√≥n.\n\n\n1.1.2.2 El operador pipe (|>) del paquete {magrittr}\nEl operador pipe nos permite concatenar funciones que utilizan como entrada los mismos datos. El principio de operaci√≥n es el siguiente, supongan que nosotros queremos cargar un dataset y aplicarle la funci√≥n summary. Esto lo podemos hacer simplemente cargando el dataset en una l√¨nea de c√≥digo y ejecutanco la funci√≥n summary() en la siguiente.\n\ndata <- read_csv(\"../data/summer.csv\")\n#> Rows: 31165 Columns: 9\n#> ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Delimiter: \",\"\n#> chr (8): City, Sport, Discipline, Athlete, Country, Gender, Event, Medal\n#> dbl (1): Year\n#> \n#> ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n#> ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nsummary(data)\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\nPero, tambi√©n podemos aprovechar el operador pipe y hacer todo en una √∫nica l√≠nea de c√≥digo.\n\nread_csv(\"../data/summer.csv\") |> summary()\n#> Rows: 31165 Columns: 9\n#> ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Delimiter: \",\"\n#> chr (8): City, Sport, Discipline, Athlete, Country, Gender, Event, Medal\n#> dbl (1): Year\n#> \n#> ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n#> ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\nAl dejar vac√≠o el par√©ntesis de la funci√≥n summary(), la misma va a tomar como variable de entrada a la que est√° antes del operador pipe, es decir, a la que antes llamamos data. En el caso que la funci√≥n summary() tuviera m√°s de una variable de entrada, lo que viene antes del pipe tomar√≠a el lugar de la primera de ellas.\nSi bien esta funcionalidad parece algo que complica las cosas y que no trae demasiados beneficios con un ejemplo tan simple, m√°s adelante veremos que puede ser de gran utilidad, ayudando a disminuir la cantidad de l√≠nes de c√≥digo y de variables intermedias.\n\n\n1.1.2.3 Dplyr y sus verbos\nUna de las cosas m√°s √∫tiles del Tidyverse para el tipo de procesamiento de datos que vamos a llevar a cabo en este libro son los verbos de dplyr. Estas funciones no permiten agregar columnas, resumir la informaci√≥n, filtrar filas, seleccionar columnas, etc. Y todas estas acciones las podemos hacer en la base de datos completa o en una parte de ella agrupada de acuerdo a alg√∫n criterio. Vayamos de a poco.\nhttps://dplyr.tidyverse.org/\n\n\n1.1.2.4 TidyR, el paquete para ordenar tus datos\n\n\n1.1.2.5 ggplot2 o c√≥mo hacer figuras que sean la envidia de tu compa√±ero de escritorio"
  },
  {
    "objectID": "intro_R.html#cierre",
    "href": "intro_R.html#cierre",
    "title": "1¬† R y el tidyverse",
    "section": "1.2 Cierre",
    "text": "1.2 Cierre\nComo vimos brevemente en este cap√≠tulo, los paquetes del tidyverse son una herramineta important√≠sima para el an√°lisis de datos utilizando R. Para m√°s detalles sobre estas funcionalidades les recomendamos la gu√≠a de Hadley Wickham(Wickham et al. 2019) o, si ya se quieren sumergir de lleno en el mundo del an√°lisis de datos con R, este fant√°stico libro [Wickham, √áetinkaya-Rundel, and Grolemund (2023)]3. Es decir, sin tener que cargar ning√∫n paquete de funciones adicional..3¬†Disponible gratis online en este link https://r4ds.had.co.nz/.\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse.‚Äù Journal of Open Source Software 4 (43): 1686.\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O‚ÄôReilly Media, Inc.\"."
  },
  {
    "objectID": "intro_stat.html#variables-aleatorias",
    "href": "intro_stat.html#variables-aleatorias",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.1 Variables aleatorias",
    "text": "2.1 Variables aleatorias\nWasserman(Wasserman 2004) nos dice que una variable aleatoria es un mapeo entre el espacio de eventos y los n√∫meros reales (\\(X:\\Omega \\rightarrow \\mathbb{R}\\)). Momento cerebrito ¬øEsto que quiere decir? En t√©rminos pr√°cticos, lo que implica es definici√≥n es que una variable aleatoria nos da un n√∫mero para cada evento del posible espacio de eventos.\nVamos con un ejemplo. Supongan que tiramos una moneda justa dos veces y tenemos la variabla aleatoria \\(X\\) que cuenta la cantidad de caras (H)1. Los posibles eventos \\(\\omega\\) del espacio de eventos \\(\\Omega\\) son \\(\\Omega = \\{ TT, TH, HT, HH \\}\\). En este caso, la variable aleatoria \\(X\\) va a tomar los valores \\(X = \\{ 0, 1, 1, 2\\}\\) para cada \\(\\omega\\). Esto, en resumidas cuentas, es lo que hace una variable aleatoria.1¬†Del ingl√©s Head y ceca sera T del ingl√©s Tail.\nEl ejemplo anterior se trata de una variable aleatoria discreta, es decir, que s√≥lo puede tomar algunos valores posibles, pero tambi√©n existen variables aleatorias continuas como por ejemplo la altura de una nueva persona que nace."
  },
  {
    "objectID": "intro_stat.html#probabilidad",
    "href": "intro_stat.html#probabilidad",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.2 Probabilidad",
    "text": "2.2 Probabilidad\nPara una variable como la definida en el ejemplo anterior podemos definir f√°cilmente su probabilidad de ocurrencia. Debido que la moneda es justa, todos los eventos de \\(\\Omega\\) son equiprobables, y es por eso que podemos definir:\n\n\n\n\\(\\omega\\)\n\\(X(\\omega)\\)\n\\(P({\\omega})\\)\n\n\n\n\nTT\n0\n1/4\n\n\nTH\n1\n1/4\n\n\nHT\n1\n1/4\n\n\nHH\n2\n1/4\n\n\n\nSin embargo, el concepto de probabilidad es algo complejo, pero, como esto no es un curso de probabilidad, vamos a confiar en que ustedes ya lo traen claro. Si tienen coraje puede ir a leer el cap√≠tulo 1 de (Wasserman 2004) y si quiere algo m√°s terrenal pueden ir a ver el repaso de probabilidad de (Cunningham 2021) (disponible online).\n\nCunningham, Scott. 2021. Causal inference: The mixtape. Yale university press.\nCuando las variables aleatorias son continuas la cosa se complica un poco m√°s ya que \\(P(X=c)\\), es decir, la probabilida de que una variable tome un valor dado, es cero. Esto lo vamos a repensar un poco en la siguiente secci√≥n, cuando definamos lo que nos importa para este libro: Las funci√≥nes de densidad y de distribuci√≥n."
  },
  {
    "objectID": "intro_stat.html#eventos-y-probabilidad-condicional",
    "href": "intro_stat.html#eventos-y-probabilidad-condicional",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.3 Eventos y probabilidad condicional",
    "text": "2.3 Eventos y probabilidad condicional\n\n\nEn construcci√≥n üöß"
  },
  {
    "objectID": "intro_stat.html#probabilidad-total",
    "href": "intro_stat.html#probabilidad-total",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.4 Probabilidad total",
    "text": "2.4 Probabilidad total\nAhora imaginemos que pasa si queremos calcular la probabilidad de B (\\(P(B)\\)). Bueno, para esto tendr√≠amos que considerar la probabilidad de que ocurra B dado que ocurri√≥ S y junto con la probabilidad de B dado que NO ocurri√≥ S. A su vez, cada una de estas probabilidades deber√≠amos pesarlas por la probabilidad de que ocurra o no A. Esto ser√≠a2:2¬†Recordemos que \\(\\neg\\) es el s√≠mbolo l√≥gico de la negaci√≥n.\n\\[\nP(B) = P(B|A) \\times P(A) +  P(B|\\neg A) \\times P(\\neg A)\n\\tag{2.1}\\]\nPensemosun ejemplo. Imaginemos que una perona tiene que completar un trabajo de jardiner√≠a (evento B) en un d√≠a. La probabilidad de que termine este trabajo si llueve (evento A) es \\(0.35\\) y la probabilidad de que lo termine si no llueve es de \\(0.95\\). Si la probabilidad de que llueva es \\(P(A)=0.4\\) ¬øCu√°l es la probabilidad (\\(P(B)\\)) de que el trabajo se complete en un d√≠a? Echemos mano a la f√≥rmula de probabilidad total:\n\\[\n\\begin{array}\n_P(B) &=& P(B|A) \\times P(A) + P(B|\\neg A) \\times P(\\neg A) \\\\\n&=& 0.35 \\times 0.4 + 0.95 \\times 0.6 \\\\\n&=& 0.71\n\\end{array}\n\\tag{2.2}\\]\nEntonces, la probabilidad de completar el trabajo en un d√≠a es \\(P(B)=0.71\\).\nPor √∫ltimo, cuando tenemos muchas condiciones, podemos definir de forma general a la probabilidad total como:\n\\[\nP(B) = \\sum_n P(B|A_n) P(A_n)\n\\tag{2.3}\\]"
  },
  {
    "objectID": "intro_stat.html#teorema-de-bayes",
    "href": "intro_stat.html#teorema-de-bayes",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.5 Teorema de Bayes",
    "text": "2.5 Teorema de Bayes\nAhora que ya llegamos a la f√≥rmula de Bayes a partir de las definiciones de probabilidad total podemos tomar prestado un ejemplo de (Herzog, Francis, y Clarke 2019): Tenemos un test para identificar si somos portadores de un virus (llam√©moslo IKV). Este test tiene una sensibilidad de 99.99% y una especificidad de 99.99%. Es decir, la probabilidad de que el test de positivo, dado que tenemos el virus (\\(P(T^+|IKV)\\)) es de 0.9999, y lo mismo ocurre para la probabilidad de que el test de negativo en caso de que NO tengamos el virus (\\(P(T^-|\\neg IKV)\\)). Sabemos tambi√©n que la incidencia del virus IKV es de 1 en 10000.\n\nHerzog, Michael H, Gregory Francis, y Aaron Clarke. 2019. Understanding statistics and experimental design: how to not lie with statistics. Springer Nature.\nSupongamos que somos elegidos aleatoriamente para realizarnos el test y este da positivo ¬øQu√© probabilidad de ser portadores del virus tenemos (\\(P(IKV|T^+)\\))? La primera respuesta que se nos viene es 0.9999 ¬øVerdad? Pero, si estuvimos prestando atenci√≥n, ya a esta altura debemos saber que para invertir la condicionalidad de una probabilidad tenemos que acudir al bueno de Bayes. O sea:\n\\[\nP(IKV|T^+) = \\frac{P(T^+|IKV) \\times P(IKV)}{P(T+)}\n\\tag{2.4}\\]\ndonde \\(P(T^+|IKV) = 0.9999\\) y \\(P(IKV) = 1/10000 = 0.0001\\). Adem√°s, echando mano a la definici√≥n de probabilidad total podemos calcular \\(P(T+)\\) como:\n\\[\nP(T+) = P(T^+|IKV) \\times P(IKV) + P(T^+|\\neg IKV) \\times P(\\neg IKV)\n\\tag{2.5}\\]\ndonde \\(P(T^+|\\neg IKV) = 1-0.9999\\) y \\(P(\\neg IKV) = 1-0.0001\\). Reemplazando todos los valores tenemos que:\n\\[\n\\begin{array}\n_P(IKV|T^+) &=& \\frac{P(T^+|IKV) \\times P(IKV)}{P(T+)}\\\\\n&=& \\frac{P(T^+|IKV) \\times P(IKV)}{P(T^+|IKV) \\times P(IKV) + P(T^+|\\neg IKV) \\times P(\\neg IKV)} \\\\\n&=& \\frac{0.9999 \\times 0.0001}{0.9999 \\times 0.0001 + (1-0.9999) \\times (1-0.0001)} \\\\\n&=& \\frac{0.9999 \\times 0.0001}{0.9999 \\times 0.0001 + 0.0001 \\times 0.9999} \\\\\n&=& 0.5\n\\end{array}\n\\tag{2.6}\\]\n¬øQu√©? ¬øEsto significa que si el test me da positivo solo tengo un 0.5 de probabilidad de tener el virus? ¬øEsto quiere decir que los tests no sirven para nada? Momento, analicemos un poco al resultado al que llegamos. Lo que nos dice esta cuenta es que, una vez que el test nos da positivo, a pesar de lo sensible del test y por lo ‚Äúraro‚Äù de la portaci√≥n del virus, nuestra probabilidad de ser portadores es de 0.5. Pero, ¬øY nuestra probabilidad de ser portadores si el test nos da negativos? Hagamos la cuenta:\n\\[\n\\begin{array}\n_P(IKV|T^-) &=& \\frac{P(T^-|IKV) \\times P(IKV)}{P(T-)}\\\\\n&=& \\frac{P(T^-|IKV) \\times P(IKV)}{P(T^-|IKV) \\times P(IKV) + P(T^-|\\neg IKV) \\times P(\\neg IKV)} \\\\\n&=& \\frac{(1-0.9999) \\times 0.0001}{(1 - 0.9999) \\times 0.0001 + (0.9999) \\times (1-0.0001)} \\\\\n&=& 1E-8\n\\end{array}\n\\tag{2.7}\\]\nOK, ahora la cosa tiene m√°s sentido. O sea, el test es bastante bueno para decirnos cuando no somos portadores y dando negativo, el problema es cuando da positivo. En este caso tenemos que preocuparnos, pero, como vimos anteriormente, la probabilidad de ser portadores es de apenas 0.5.\nHay una soluci√≥n m√°s simple para esto y es la que deben estar pensando ustedes: ¬øY si me hacen un segundo test? ¬°BINGO! Calculemos r√°pidamente la probabilidad de estar infectados si nos testean por segunda vez:\n\\[\nP(IKV|T^{2+}) = \\frac{0.9999^2 \\times 0.0001}{0.9999^2 \\times 0.0001 + 0.0001^2 \\times 0.9999} = 0.9999\n\\tag{2.8}\\] Ahora s√≠, si somos testeados por segunda vez, la probabilidad de ser portadores dado que tenemos dos resultados positivos trepa a 0.9999. Nos podemos quedar tranquilos.\nPara cerrar, me gustar√≠a que pensemos un poco en una palabra MUY importante que se dijo en el enunciado del problema: Aleatoriamente. En muchos de los casos en los que nos testeamos para ver si somos portadores de un virus, lo hacemos porque tenemos alg√∫n tipo de presunci√≥n de que podemos serlo (por ejemplo, tenemos s√≠ntomas). ¬øCu√°l creen que ser√≠a la probabilidad que se modifica en la f√≥rmula? Exacto, \\(P(IKV)\\), ya que ser√≠a m√°s bien \\(P(IKV|s√≠ntomas)\\)."
  },
  {
    "objectID": "intro_stat.html#esperanza",
    "href": "intro_stat.html#esperanza",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.6 Esperanza",
    "text": "2.6 Esperanza\nLa esperanza de una variable aleatoria \\(X\\), a veces tambi√©n llamada media poblacional, es simplemente la suma pesada de todos sus valores posibles. No debemos confundir la esperanza con el promedio muestral, aunque, como veremos en breve, para algunos casos el primero es un estimador insesgado del segundo.\nLa esperanza de una variable aleatoria discreta se define como:\n\\[\nE(X) = \\sum_{1}^\\infty x_i p(x_i)\n\\tag{2.9}\\] En este caso es muy claro la naturaleza de promedio pesado, ya que a cada valor posible de \\(X\\) lo estamos pesando por su probabilidad. Sin embargo, para una variable aleatoria continua, en la que no tenemos definida una probabilidad puntual \\(p(x_i)\\) sino una funci√≥n de densidad \\(f(x)\\), la definici√≥n es la siguiente:\n\\[\nE(X) = \\int_{-\\infty}^\\infty x f(x) dx\n\\tag{2.10}\\]\nComo resulta esperable, la suma se transforma en una integral y la probabilidad puntual se reemplaza por \\(f(x)\\).\nAlgunas propiedades importantes de la esperanza son:\n\\[\n\\begin{array}\n_E(aX+b) & = & aE(X) + b\\\\\nE(\\sum_{i=1}^n X_i) & = & \\sum_{i=1}^nE(X_i)\n\\end{array}\n\\tag{2.11}\\]\nPor √∫ltimo y a modo de aviso, advertencia y amenaza, recordemos que \\(E(X)^2 \\neq E(X^2)\\)."
  },
  {
    "objectID": "intro_stat.html#varianza-y-covarianza",
    "href": "intro_stat.html#varianza-y-covarianza",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.7 Varianza y covarianza",
    "text": "2.7 Varianza y covarianza\nLa varianza nos da una idea de la variabilidad de los procesos aleatorios que generan una variable aleatorio3. La varianza de la variable aleatoria \\(X\\) se define como:3¬†Dato que ser√° de vital importancia para sentar las bases de la inferencia estad√≠stica.\n\\[\nV(X) = \\sigma^2 = E \\left[ (X - E(X))^2 \\right]\n\\tag{2.12}\\]\nY se puede demostrar que:\n\\[\nV(X) = E(X^2) - E^2(X)\n\\tag{2.13}\\]\nPor otro lado, la covarianza es mide la cantidad de dependencia lineal entre dos variables aleatorias. La msma se define como\n\\[\nCov(X,Y) = E(XY) - E(X)E(Y)\n\\tag{2.14}\\]\nSi \\(Cov(X,Y)>0\\), esto indica que las dos variables se mueven en la misma direcci√≥n, mientras que si \\(Cov(X,Y)<0\\) esto indica que ambas variables se mueven en direcciones opuestas."
  },
  {
    "objectID": "intro_stat.html#correlaci√≥n",
    "href": "intro_stat.html#correlaci√≥n",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.8 Correlaci√≥n",
    "text": "2.8 Correlaci√≥n\nLa correlaci√≥n es la versi√≥n m√°s amigable de la covarianza. ¬øPor qu√©? Porque nos dice cu√°nto covar√≠an dos variables independiz√°ndose de la varianza de cada una de ellas, es decir, normalizando. Esto la convierte en una medida muy relevante e informativa. Si tenemos dos variables aleatorias \\(X\\) e \\(Y\\), la correlaci√≥n de se define como la covarianza es sus versiones estandarizadas:\n\\[\n\\begin{array}\n_W &=& \\frac{X-E(X)}{\\sqrt{V(X)}} \\\\\nZ &=& \\frac{Y-E(Y)}{\\sqrt{V(Y)}}\n\\end{array}\n\\tag{2.15}\\]\nDe la siguiente forma:\n\\[\n\\begin{array}\n_Corr(X,Y) &=& Cov(W,Z) \\\\\n&=&  Cov(\\frac{X-E(X)}{\\sqrt{V(X)}}, \\frac{Y-E(Y)}{\\sqrt{V(Y)}}) \\\\\n&=&  \\frac{1}{\\sqrt{V(X)}} \\frac{1}{\\sqrt{V(Y)}}  Cov(X-E(X),Y-E(Y)) \\\\\n&=&  \\frac{Cov(X,Y)}{\\sqrt{V(X) V(Y)}}\n\\end{array}\n\\tag{2.16}\\]\nUsamos la propiedad de la covarianza que dice que:\n\\[\nCov(X+a, Y+b) = Cov(X,Y) + Cov(X,b) + Cov(a,Y) + Cov(a,b)\n\\tag{2.17}\\]\nY como \\(a\\) y \\(b\\) son constantes (en nuestro caso esperanzas), el √∫nico t√©rmino que sobrevive es \\(Cov(X,Y)\\). Esta magnitud es tambi√©n conocida como el coeficiente de correlaci√≥n \\(\\rho\\).\nCon esta definici√≥n en la mano, si yo les digo que dos variables \\(X\\) e \\(Y\\) tienen una covarianza de \\(14.988\\) no les dice mucho, ¬øNo? Ahora, si les digo que el coeficiente de correlaci√≥n es de \\(0.788\\) probablemente entiendan r√°pidamente que ambas variables est√°n muy relacionadas4.4¬†Si quieren divertirse y de paso convertise en ases de la determinaci√≥n del coeficiente de correlaci√≥n a oj√≠metro les recomiendo este juegazo. Una estudiante ostenta el abultado r√©cord de \\(231\\) puntos ¬øLa pasaste?\nVeamos este ejemplo con n√∫meros y de paso repasemos como se calcula la correlaci√≥n en R:\n\n\nVer el c√≥digo\nset.seed(1234)\nx = rnorm(1000, 20, 4)\ny = x*.9 + rnorm(1000, 2, 3)\ncat(paste(\"La covarianza entre X e Y es\", round(cov(x,y), 3)))\n#> La covarianza entre X e Y es 14.988\ncat(paste(\"La correlaci√≥n entre X e Y es\", round(cor(x,y), 3)))\n#> La correlaci√≥n entre X e Y es 0.788\n\n\n\n\n\n\n\nRelaci√≥n lineal entre X e Y.\n\n\n\n\nEs muy importante tener en cuenta que el cueficiente de correlaci√≥n nos dice cu√°n linealmente relacionadas est√°n las variables. Veamos esto con un ejemplito:\n\n\nVer el c√≥digo\nset.seed(1234)\nx = runif(1000, -1, 1)\ny = x^2 + .1*runif(1000, -1, 1)\ncat(paste(\"La covarianza entre X e Y es\", round(cov(x,y), 3)))\n#> La covarianza entre X e Y es 0.01\ncat(paste(\"La correlaci√≥n entre X e Y es\", round(cor(x,y), 3)))\n#> La correlaci√≥n entre X e Y es 0.055\n\n\n\n\n\n\n\nRelaci√≥n no lineal entre X e Y.\n\n\n\n\nEn este caso vemos que claramente hay uan relaci√≥n entre X e Y (no son una nube de puntos sin estructura) pero como esta relaci√≥n no es lineal (cuadr√°tica en nuestro caso), el coeficiente de correlaci√≥n es cercano a cero.\nFinalmente, tengamos en cuenta que el coeficiente de correlaci√≥n puede tomar valores entre \\(-1\\) y \\(1\\). Una correlaci√≥n positiva indica que las variables var√≠an de la misma manera (si aumenta una aumenta la otra) y lo contrario ocurre con una correlaci√≥n negativa (si aumenta una disminuye la otra. Cuanto m√°s cerca est√© el coeficiente de \\(1\\) o, m√°s fuerte es la relaci√≥n lineal."
  },
  {
    "objectID": "intro_stat.html#poblaci√≥n-y-muestra",
    "href": "intro_stat.html#poblaci√≥n-y-muestra",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.9 Poblaci√≥n y muestra",
    "text": "2.9 Poblaci√≥n y muestra\nAc√° usar una versi√≥n de la figurita de All of statistics que pone la generaci√≥n de los datos en el dominio de la probabilidad y la estimaci√≥n de estos par√°metros en el dominio de la estad√≠stica. Me parece una forma ideal de empezar a hablar de qu√© queremos hacer con la estad√≠stica.\nVamos con un ejemplo que nos puede ayudar a entender de qu√© hablamos cuando hablamos de estimaci√≥n. Supongamos que conocemos distribuci√≥n de la altura de la poblaci√≥n de varones en Argentina. No estamos hablando de calcular el promedio de la altura de los varones sino de que conocemos la funci√≥n de densidad de la cual la altura de cada var√≥n es una muestra. Si no queda del todo claro respiren hondo y esperen un poco que ya se va a ir aclarando. Entonces, la altura de los varones de Argentina tiene una distribuci√≥n normal con media en cm. de \\(\\mu_{varones} = 175\\) y una desviaci√≥n est√°ndar \\(\\sigma_{varones} = 7\\), o, como ya aprendimos a decir: \\(H_{varones} \\sim \\mathcal{N}(\\mu_{varones},\\sigma^2_{varones}) = \\mathcal{N}(175, 49)\\)5. A continuaci√≥n podemos ver la funci√≥n de densidad:5¬†h del ingl√©s height.\n\n\n\n\n\nFunci√≥n de densidad de probabilidad de la variabla aleatoria H (altura de los varones)\n\n\n\n\nAhora bien, en la figura podemos ver la funci√≥n \\(f_X(x)\\) junto con una l√≠nea vertical que nos indica la media y dos l√≠neas que nos indican los percentiles \\(2.5\\) y \\(97.5\\), es decir, que contienen el 95% de la masa de probabilidad. Todo esto es muy lindo pero estamos jugando a ser dios (o el Doctor Manhattan, o en lo que ustedes crean). Es imposible conocer los par√°metros de esta distribuci√≥n pero lo que s√≠ podemos hacer en la pr√°ctica es estimarlos. Estimar los par√°metros de un modelo es el pan y manteca de la inferencia estad√≠stica y el data mining. Como podemos ver en esta hermosa figura de Wasserman(Wasserman 2004), la teor√≠a de probabilidad nos ayuda a definir modelos para la generaci√≥n de datos y la inferencia estad√≠stica nos ayuda a estimar estos par√°metros.\n\nWasserman, Larry. 2004. All of statistics: a concise course in statistical inference. Springer Science & Business Media.\nHay diversas formas de encontrar estimadores para los par√°metros de un modelo (por ejemplo, m√©todo de los momentos, m√°xima verosimilitud, etc.) pero entenemos que eso excede los contenidos de este curso. Sin embargo, para estimar todos conocemos los estimadores de los par√°metros poblacionales \\(\\mu\\) y \\(\\sigma^2\\). Claro, el promedio \\(\\bar{x}\\) y el desv√≠o muestral \\(\\hat{S}^2\\):\n\\[\n\\begin{array}\n\\\\\\bar{x} & = & n^{-1} \\sum_{i=1}^n x_i\\\\\n\\hat{S}^2 & = & (n-1)^{-1} \\sum_{i=1}^n (x_i\n\\end{array}\n\\tag{2.18}\\]\nSimulemos tres experimento tomando 10, 50 y 100 mediciones (\\(n\\)) y veamos los histogramas de estas muestras y sus estimaciones de \\(\\mu\\) y \\(\\sigma\\):\n\n\n\n\n\n\n\n\n\nComo es de esperarse, podemos ver que al aumentar \\(n\\), la estimaci√≥n de los par√°metros poblacionales es mejor. Pero tenemos que tener esta idea en mente, cada vez que tomamos una muestra podemos estimar un par√°metro de la poblaci√≥n y hasta hacer inferencias estad√≠sticas sobre el mismo, pero NUNCA lo vamos a conocer.\nAlgo importante cuando usemos un estimador es que este sea consistente, lo que implica que si aumentamos \\(n\\) al infinito, el estimador converge al valor del par√°metro (en este caso el promedio muestral para \\(n \\to \\infty\\) tiende a la media poblacional \\(\\mu\\)). Decimos entonces que un estimador converge en probabilidad a un determinado par√°metro. Como usuarios de la estad√≠stica esto nos va a venir masticado y no nos vamos a tener que preocupar tanto pero es bueno tenerlo en mente cuando hablamos de estimadores y estimaciones."
  },
  {
    "objectID": "potential_outcomes.html",
    "href": "potential_outcomes.html",
    "title": "3¬† Potential outcomes",
    "section": "",
    "text": "En construcci√≥n üöß"
  },
  {
    "objectID": "potential_outcomes.html#sdf",
    "href": "potential_outcomes.html#sdf",
    "title": "3¬† Potential outcomes",
    "section": "3.1 Sdf",
    "text": "3.1 Sdf"
  },
  {
    "objectID": "dags.html",
    "href": "dags.html",
    "title": "4¬† Grafos ac√≠clicos dirigidos (DAGS)",
    "section": "",
    "text": "Los grafos ac√≠clicos dirigidos 1son una herramienta para representar visualmente las relaciones causales presentes en nuestro dise√±o experimental, ni m√°s ni menos..1¬†A partir de ahora DAGS, del ingl√©s Directed acyclic graphs."
  },
  {
    "objectID": "dags.html#definiciones-caracter√≠sticas-y-ejemplos",
    "href": "dags.html#definiciones-caracter√≠sticas-y-ejemplos",
    "title": "4¬† Grafos ac√≠clicos dirigidos (DAGS)",
    "section": "4.1 Definiciones, caracter√≠sticas y ejemplos",
    "text": "4.1 Definiciones, caracter√≠sticas y ejemplos\nUn DAG es una representaci√≥n gr√°fica de una cadena de efectos causales. Los nodos (los circulitos o cuadraditos que vamos a ver m√°s adelante) representan variables aleatorias y las flechas que los unen representan la relaci√≥n causal que se mueve de una variable a la otra en la direcci√≥n intuitiva de la flecha. Por ejemplo, pensemos que queremos estudiar el efecto de tomar una aspirina en la intensidad de nuestro dolor de cabeza cuando nos duele la cabeza. Tenemos dos variables Aspirina (A) e Intensidad del dolor (I). Esta relaci√≥n la podemos expresar en el siguiente DAG:\n\n\n\n\n\nDAG que representa la relaci√≥n entre tomar una aspirina (A) y la intensidad del dolor (I).\n\n\n\n\n\n\n\n\n\n\nDAGS en R\n\n\n\nPara realizar los DAGS que aparecen en este cap√≠tulo utilizamos la librer√≠a {ggdag}. Para esto primero debemos hacer un esquema de nuestro DAG en Daggity y luego copiar parte de lo generado en la definici√≥n de nuestro DAG en R. En el desarrollo del cap√≠tulo utilizaremos algunas de las herramientas de {ggdag}, varias de las cuales pueden encontrar en este tutorial. Sin embargo, para una revisi√≥n pormenorizada de sus funciones recomiendo repasar la documentaci√≥n del mismo.\n\n\n¬øAs√≠ de simple? S√≠ y no ¬øQu√© pasa cuando las cosas se empiezan a complicar? Pensemos en el cl√°sico ejemplo del mantra ‚Äúcorrelaci√≥n no implica causalidad‚Äù la relaci√≥n entre venta de helados (Hel) y accidentes por mordidas de tibur√≥n en Australia(Sh). Si nosotros plante√°ramos que el aumento de la venta de helados causa el aumento de los accidentes no tendr√≠a mucho sentido, ¬øNo?. Entonces, ¬øQu√© est√° pasando? ¬øQu√© pinta tiene el DAG? Bueno, como ya comentamos cuando hablamos de correlaci√≥n en este caso lo que tenemos es una com√∫n a ambos fen√≥menos que, a modo de simplificar, la podr√≠amos resumir como la temperatura (T). Entonces, cuando sube la temperatura sube la venta de helados, pero tambi√©n los accidentes por mordidas de tibur√≥n. Una posible relaci√≥n causal entre ambas variables podr√≠a ser esta:\n\n\n\n\n\nDAG que representa la relaci√≥n entre las ventas de helados (Hel), los accidentes por mordida de tibur√≥n (Sh) y la temnperatura (T) en las playas de Australia.\n\n\n\n\nTodo muy lindo, pero ¬øPara qu√©?"
  },
  {
    "objectID": "dags.html#el-criterio-de-las-puertas-de-atr√°s",
    "href": "dags.html#el-criterio-de-las-puertas-de-atr√°s",
    "title": "4¬† Grafos ac√≠clicos dirigidos (DAGS)",
    "section": "4.2 El criterio de las puertas de atr√°s",
    "text": "4.2 El criterio de las puertas de atr√°s\nUna de las principales ventajas de plantear un DAg para estudiar una relaci√≥n causal es que nos permite ajustar un modelo a partir del cual, lo que estamos estimando en uno de sus par√°metros es un estimador de la relaci√≥n causal que queremos estudiar. Empecemos planteando los caminos posibles para llegar desde Hel a Sh. En este caso ser√≠an dos:\n\\[\n\\begin{array}\n_Hel    \\longrightarrow Sh \\\\\nHel \\longleftarrow T \\longrightarrow Sh\n\\end{array}\n\\]\nEl primero es lo que se llama un camino directo y es lo relaci√≥n causal que queremos estudiar. Por otro lado, el segundo (\\(Hel \\leftarrow T \\rightarrow Sh\\)) es lo que se llama una puerta de atr√°s y lo ident√≠ficamos porque, al menos en alguna de sus relaciones causales, la flechita va para la izquierda. Identificar estos caminos puerta de atr√°s es una parte fundamental de controlar la variabilidad esp√∫rea en nuestras relaciones causales. En particular en este ejemplo, tenemos un claro confusor y lo que queremos es controlar por T.\nSimulemos esta relaci√≥n y veamos que pasa.\n\n\nVer el c√≥digo\nset.seed(1414)\nhelados <- tibble(\n  Temperatura = rnorm(1000, 20, 5),\n  Helados     = 1 + 1*Temperatura + rnorm(1000),\n  Ataques     = 1 + 2*Temperatura + 0*Helados + rnorm(1000)\n)\n\n\nVeamos que Temperatura es una variable aleatoria con distribuci√≥n normal con media \\(20\\) y desviaci√≥n est√°ndar \\(5\\). Tomamos \\(1000\\) muestras de la misma y despu√©s generamos las variables Helados y Ataques como una combinaci√≥n lineal de Temperatura m√°s un error aleatorio de media \\(0\\) y desviaci√≥n est√°ndar \\(1\\). En la definici√≥n de Ataques podemos ver expl√≠citamente que la influencia de Helados en Ataques es \\(0\\). Sin embargo, miremos la relaci√≥n que existe entre ambas variables:\n\n\n\n\n\nDAG que representa la relaci√≥n entre las ventas de helados (Hel), los accidentes por mordida de tibur√≥n (Sh) y la temnperatura (T) en las playas de Australia.\n\n\n\n\nVemos que ambas variables est√°n altamente correlacionadas, de hecho, su r de Pearson vale 0.977. Sin embargo, nosotros sabemos que esa correlaci√≥n es esp√∫rea, que no hay una relaci√≥n causal entre ventas de helados y ataques de tibur√≥n y que algo tenemos que hacer. Hemos escuchado muchas veces que lo que tenemos que hacer es ‚Äúcontrolar‚Äù por la temperatura, lo que en el contexto de la regresi√≥n lineal no significa otra cosa que agergar Temperatura como una covariable. Ajustemos dos modelos de regresi√≥n, uno con la Temperatura como covariable y uno sin y comparemos las estimaciones de los efectos causales (\\(\\hat\\beta_H\\))2 :2¬†Recuerden que el diagrama causal no es un problema estad√≠stico sino m√°s bien se plantea previo a cualquier consideraci√≥n estad√≠stica, usando como insumo el conocimiento del dominio que tenemos.\n\\[\n\\begin{array}\n_lm_1&:& Ataques_i = \\alpha + \\beta_{H} Helados_i + \\epsilon_i \\\\\nlm_2&:& Ataques_i = \\alpha + \\beta_{H} Helados_i +  \\beta_{T} Temperatura_i + \\tau_i\n\\end{array}\n\\] En la siguiente tabla podemos ver las estimaciones de \\(\\hat\\beta_H\\) para cada uno de los modelos:\n\n\nEstimaciones de los par√°metros de los modelos lm1 y lm2 definidos anteriormente.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtaques\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSin controlar por Temp.\n\n\n\n\n\n\n\n\n\n\n\n\nControlando por Temp.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\n\n\n\n\n\n\n\n\n(2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHelados\n\n\n\n\n\n\n\n\n\n\n\n\n1.928***\n\n\n\n\n\n\n\n\n\n\n\n\n-0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\n(0.032)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTemperatura\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.003***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.033)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n\n\n\n\n\n\n\n\n\n\n0.602**\n\n\n\n\n\n\n\n\n\n\n\n\n1.015***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.291)\n\n\n\n\n\n\n\n\n\n\n\n\n(0.134)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n\n\n\n\n\n1,000\n\n\n\n\n\n\n\n\n\n\n\n\n1,000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR2\n\n\n\n\n\n\n\n\n\n\n\n\n0.954\n\n\n\n\n\n\n\n\n\n\n\n\n0.990\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjusted R2\n\n\n\n\n\n\n\n\n\n\n\n\n0.954\n\n\n\n\n\n\n\n\n\n\n\n\n0.990\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidual Std. Error\n\n\n\n\n\n\n\n\n\n\n\n\n2.246 (df = 998)\n\n\n\n\n\n\n\n\n\n\n\n\n1.032 (df = 997)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF Statistic\n\n\n\n\n\n\n\n\n\n\n\n\n20,826.210*** (df = 1; 998)\n\n\n\n\n\n\n\n\n\n\n\n\n51,243.220*** (df = 2; 997)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\n\n\n\n\n\n\n\n\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo podemos ver, si no controlamos por Temperatura , la estimaci√≥n de \\(\\hat\\beta_H\\) tiene un valor cercano a \\(1\\), mientras que si controlamos por Temperatura tiene un valor cercano a \\(0\\) lo que sabemos es el valor ‚Äúreal‚Äù del par√°metro \\(\\beta_H\\). Todo muy lindo pero, ¬øQu√© tiene que ver esto con los DAGS? Bueno, cuando planteamos un DAG existe algo que se llama el criterio de las puertas traseras que dice que para estimar el efecto causal que nos interesa debemos ‚Äúcerrar‚Äù todas las puertas traseras que conectan la causa y elefecto. Y ‚Äúcerrar‚Äù en este contexto es simplemente controlar por la variabilidad de alguna de las variables presentes en la puerta trasera. En este caso, controlando por Temperatura estamos cerrando la √∫nica puerta trasera, por lo tanto, estamos estimando el verdadero efecto causal que nos interesa."
  },
  {
    "objectID": "dags.html#colliders",
    "href": "dags.html#colliders",
    "title": "4¬† Grafos ac√≠clicos dirigidos (DAGS)",
    "section": "4.3 Colliders",
    "text": "4.3 Colliders\nHasta ahora tuvimos que lidiar casi √∫nicamente con confusores pero existe otro tipo de variables en el contexto de un camino causal que se denomina collider. Vemos un ejemplo y apliquemos el criterio de las puertas traseras. Pensemos el siguiente ejemplo. Supongamos que queremos estudiar el efecto del factor de riesgo edad (Age) en la infecci√≥n de COVID-19 (Cov), pero lo hacemos a trav√©s de datos voluntarios recavados por una aplicaci√≥n m√≥vil (App). Un DAG muy simplicado que podr√≠amos plantear es el siguiente:\n\n\n\n\n\nDAG que representa la relaci√≥n entre la edad (Age), la infecci√≥n por COVID-19 (Cov) y el uso de la aplicaci√≥n m√≥vil de autoreporte (App).\n\n\n\n\nYa uqe sabemos que la edad tienen un. efecto en el uso de aplicaciones m√≥viles y, podemos suponer, que la gente que se infecta de COVID-19 tiende a reportar m√°s sus datos en la aplicaci√≥n. Ahora veamos los caminos:\n\\[\n\\begin{array}\n_Age    \\longrightarrow Cov \\\\\nAge \\longrightarrow App \\longleftarrow Cov\n\\end{array}\n\\] Repitamos el ejercicio de simulaci√≥n que utilizamos en el ejemplo de los confusores, s√≥lo que esta vez Covid es una variable dicot√≥mica y, por lo tanto, debemos muestrarla de una distribuci√≥n Bernoulli3:3¬†Esto resulta especialmente √∫til cuando los DAGS se empiezan a complicar.\n\n\nVer el c√≥digo\nset.seed(123)\ncovid <- tibble(\n  Age   = rnorm(1000, 40, 10),\n  Covid = rbinom(1000, 1, prob = 1/(1+exp(10-.25*Age))),\n  App   = 1 - 1*Age + 1*Covid +rnorm(1000)\n)\n\n\nPuede verse que la verdadera relaci√≥n entre Covid y Age (en t√©rminos de par√°metros de una regresi√≥n log√≠stica) es \\(0.25\\). Veamos como se ve la edad de los infectados y no infectados:\n\n\n\n\n\nInfectados y no infectados de COVID-19 en funci√≥n de la edad.\n\n\n\n\nSeg√∫n el criterio de las puertas traseras deber√≠amos controlar por App para as√≠ cerrar ese camino. Ajustemos estos dos regresiones log√≠sticas y veamos sus estimaciones de los par√°metros:\n\\[\n\\begin{array}\n_glm_1&:& logit(Covid_i) = \\alpha + \\beta_{Age} Age_i + \\epsilon_i \\\\\nglm_2&:& logit(Covid_i) = \\alpha + \\beta_{Age} Age_i + \\beta_{App} App_i + \\epsilon_i\n\\end{array}\n\\]\nEn la siguiente tabla podemos ver las estimaciones de \\(\\hat\\beta_{Age}\\) para cada uno de los modelos:\n\n\nEstimaciones de los par√°metros de los modelos glm1 y glm2 definidos anteriormente.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSin controlar por App\n\n\n\n\n\n\n\n\n\n\n\n\nControlando por App\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\n\n\n\n\n\n\n\n\n(2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\n\n\n\n\n\n\n\n0.243***\n\n\n\n\n\n\n\n\n\n\n\n\n1.354***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.016)\n\n\n\n\n\n\n\n\n\n\n\n\n(0.109)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.100***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.103)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n\n\n\n\n\n\n\n\n\n\n-9.787***\n\n\n\n\n\n\n\n\n\n\n\n\n-11.839***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.631)\n\n\n\n\n\n\n\n\n\n\n\n\n(0.763)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n\n\n\n\n\n1,000\n\n\n\n\n\n\n\n\n\n\n\n\n1,000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog Likelihood\n\n\n\n\n\n\n\n\n\n\n\n\n-419.336\n\n\n\n\n\n\n\n\n\n\n\n\n-344.156\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n\n\n\n\n\n\n\n\n\n\n842.672\n\n\n\n\n\n\n\n\n\n\n\n\n694.312\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\n\n\n\n\n\n\n\n\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa estimaci√≥n del efecto correcta ser√≠a la del modelos sin controlar pero ¬øPor qu√© pasa esto? Cuando tenemos un collider podemos considerar ese camino como cerrado por defecto y al controlar por √©l, ese camino se abre. Entonces, en si bien ten√≠amos dos posibles caminos causales, s√≥lo uno estaba abierto y no necesit√°bamos controlar por App. Esto se debe a que, al App no causar ninguna de mis otras dos variables, ese camino causal est√° cerrado. Para reflexionar un poco en por qu√© ese camino se abre al controlar por un collider recomiendo las reflexiones del cap√≠tulo 8 de (Huntington-Klein 2021).\n\n\n\n\n\n\nEl criterio de las puertas traseras\n\n\n\nEn resumen, el criterio de las puertas traseras nos dice que para estimar la relaci√≥n causal principal debemos cerrar todas las puertas traseras (caminos causales entre la causa y el efecto que queremos estudiar que tienen alguna flecha hacia atr√°s). Recordemos que para cerrar esos caminos debemos controlar por alguna de las variables que lo componen, agrag√°ndola como covariable a nuestro modelo estad√≠stico. Por √∫ltimo, recordemos que cuando tenemos un collider el camino est√† cerrado y al controlar por √©l lo abrimos."
  },
  {
    "objectID": "dags.html#un-ejemplo",
    "href": "dags.html#un-ejemplo",
    "title": "4¬† Grafos ac√≠clicos dirigidos (DAGS)",
    "section": "4.4 Un ejemplo",
    "text": "4.4 Un ejemplo\nAnalicemos un ejemplo que tiene un poquito de todo. En el mismo queremos estudiar la el efecto de las vitaminas (Vits) en los defectos de nacimiento (BD). Adem√°s de esta relaci√≥n, podemos identificar otras variables que podr√≠an influir en ellas: La dificultad para concebir un embarazo (DC); El cuidado pre-natal (PNC); el status socioecon√≥mico (SES); y aspectos gen√©ticos (Gen). El flujo de causalidad propuesto entre las variables puede verse representado en el siguiente DAG4:4¬†Recuerden que el diagrama causal no es un problema estad√≠stico sino m√°s bien se plantea previo a cualquier consideraci√≥n estad√≠stica, usando como insumo el conocimiento del dominio que tenemos.\n\n\nVer el c√≥digo\ndag_Vit <- dagitty::dagitty('dag {\n                          BD [outcome,pos=\"0.109,0.631\"]\n                          DC [pos=\"0.117,-1.517\"]\n                          Gen [pos=\"0.850,-0.411\"]\n                          PNC [pos=\"-0.837,-0.433\"]\n                          SES [pos=\"-1.839,-1.468\"]\n                          Vits [exposure,pos=\"-1.844,0.645\"] \n                          DC -> PNC\n                          Gen -> BD\n                          Gen -> DC\n                          PNC -> BD\n                          PNC -> Vits\n                          SES -> PNC\n                          SES -> Vits\n                          Vits -> BD\n                          }')\n\ntidy_dag <- tidy_dagitty(dag_Vit)\nggdag(tidy_dag) +\n  theme_dag()\n\n\n\n\n\nDAG que representa la relaci√≥n causal entre las vitaminas y los defectos de nacimiento.\n\n\n\n\nSi tebemos todos estos datos observados y queremos estimar el efecto causal de las vitaminas en los defectos de nacimiento, lo primero que tenemos que hacer es plantear todos los caminos abiertos. Esto lo podemos hacer a ojo, pero tambi√©n nos podemos ayudar con la funci√≥n ggdag_paths del paquete {ggdags}5. A continuaci√≥n vemos un ejemplo de uso de esta funci√≥n:5¬†Esto resulta especialmente √∫til cuando los DAGS se empiezan a complicar.\n\n\nVer el c√≥digo\ndag_Vit %>% ggdag_paths(from = \"Vits\", to = \"BD\", shadow = TRUE) +\n  theme_dag() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nUsando ggdag_collider para identificar colliders en nuestro DAG.\n\n\n\n\nPodemos ver que los caminos abiertos son:\n\\[\n\\begin{array}\n_Vits \\longrightarrow BD\\\\\nVits \\longleftarrow PNC \\longrightarrow BD\\\\\nVits \\longleftarrow PNC \\longleftarrow DC \\longleftarrow Gen \\longrightarrow BD \\\\\nVits \\longleftarrow SES \\longrightarrow PNC \\longrightarrow BD\n\\end{array}\n\\]\nPero ¬øPor qu√© no est√° abierto el camino \\(Vits \\leftarrow SES \\rightarrow PNC \\leftarrow DC \\leftarrow Gen \\rightarrow BD\\)? ¬°Exacto! Est√° cerrado, porque para ese camino PNC es un collider, ya que est√° causada tanto por SES como por DC. Resulta importante notar que una variable es un collider o no en el contexto de un camino de causalidad y no lo es siempre. De hecho, podemos ver que PNC act√∫a como un confusor en el camino \\(Vits \\leftarrow PNC \\rightarrow BD\\). La funci√≥n ggdag_collider tambi√©n nos puede ayudar a identificar un collider.\n\n\nVer el c√≥digo\ndag_Vit %>% ggdag_collider() +\n     theme_dag() +\n     scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\nUsando ggdag_collider para identificar colliders en nuestro DAG.\n\n\n\n\nEntonces tenemos cuatro caminos abiertos, el que queremos estudiar y tres puertas de atr√°ss. Sin embargo, tenemos la suerte que controlando s√≥lo por PNC Todo indica que tenemos que controlar por Diab y listo ¬øNo? Apliquemos esto agregando el par√°metro adjust_for = \"Diab\" a la funci√≥n ggdag_paths:\n\n\nVer el c√≥digo\ndag_Vit %>% ggdag_paths(from = \"Vits\", to = \"BD\",\n                       adjust_for = \"PNC\", shadow = TRUE) +\n  theme_dag() +\n  labs(hue = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nCaminos causales abiertos luego de controlar por PNC.\n\n\n\n\nSin embargo, podemos ver que, a√∫n controlando por PNC, los caminos abiertos son:\n\\[\n\\begin{array}\n_Vits \\longrightarrow BD\\\\\nVits \\leftarrow SES \\rightarrow PNC \\leftarrow DC \\leftarrow Gen \\rightarrow BD\n\\end{array}\n\\]\n¬øQu√© pas√≥? Bueno, lo que pas√≥ es que al controlar por un collider abrimos un camino que estaba cerrado. Miremos qu√© pasa si controlamos por PNC y DC:\n\n\nVer el c√≥digo\ndag_Vit %>% ggdag_paths(from = \"Vits\", to = \"BD\",\n                       adjust_for = c(\"PNC\", \"DC\"), shadow = TRUE) +\n  theme_dag() +\n  labs(hue = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nCaminos causales abiertos luego de controlar por Diab y Smok.\n\n\n\n\nAhora s√≠, el √∫nico camino abierto es \\(Vits \\rightarrow DC\\), que es la relaci√≤n causal que queremos estudiar. Finalmente, el modelo que deber√≠amos ajustar es:\n\\[\nDC_i = \\alpha + \\beta_{Vits} Vits_i + \\beta_{PNC} PNC_i + \\beta_{DC} DC_i + + \\epsilon_i\n\\]\nDonde \\(\\beta_{Vits}\\) es un estimador del efecto causal que queremos estudiar.\nPara m√°s ejemplos y detalles sobre los DAGS pueden consultar (Cunningham 2021) o (Huntington-Klein 2021). El canal de YouTube de Nick Huntington-Klein tambi√©n es un excelente recurso para profundizar sobre estos temas6.6¬†Nick es el autor de (Huntington-Klein 2021).\nHuntington-Klein, Nick. 2021. The effect: An introduction to research design and causality. Chapman; Hall/CRC.\n\n\n\n\nCunningham, Scott. 2021. Causal inference: The mixtape. Yale university press."
  },
  {
    "objectID": "exp_aleatorios.html",
    "href": "exp_aleatorios.html",
    "title": "5¬† Experimentos aleatorios",
    "section": "",
    "text": "En construcci√≥n üöß"
  },
  {
    "objectID": "exp_aleatorios.html#sdf",
    "href": "exp_aleatorios.html#sdf",
    "title": "5¬† Experimentos aleatorios",
    "section": "5.1 Sdf",
    "text": "5.1 Sdf"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cunningham, Scott. 2021. Causal Inference: The Mixtape. Yale\nuniversity press.\n\n\nHerzog, Michael H, Gregory Francis, and Aaron Clarke. 2019.\nUnderstanding Statistics and Experimental Design: How to Not Lie\nwith Statistics. Springer Nature.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. Chapman; Hall/CRC.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRubin, Donald B. 1974. ‚ÄúEstimating Causal Effects of Treatments in\nRandomized and Nonrandomized Studies.‚Äù Journal of Educational\nPsychology 66 (5): 688.\n\n\nT√∂nnies, Thadd√§us, Sabine Kahl, and Oliver Kuss. 2022. ‚ÄúCollider\nBias in Observational Studies: Consequences for Medical Research Part 30\nof a Series on Evaluation of Scientific Publications.‚Äù\nDeutsches √Ñrzteblatt International 119 (7): 107.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in\nStatistical Inference. Springer Science & Business Media.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019.\n‚ÄúWelcome to the Tidyverse.‚Äù Journal of Open Source\nSoftware 4 (43): 1686.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O‚ÄôReilly Media, Inc.\"."
  },
  {
    "objectID": "dags.html#causalidad-sin-correlaci√≥n",
    "href": "dags.html#causalidad-sin-correlaci√≥n",
    "title": "4¬† Grafos ac√≠clicos dirigidos (DAGS)",
    "section": "4.3 Causalidad sin correlaci√≥n",
    "text": "4.3 Causalidad sin correlaci√≥n\nMuchas veces escuchamos que ‚Äúcorrelaci√≥n no implica causalidad‚Äù pero ¬øCausalidad implica correlaci√≥n? Respondamos esta pregunta con un ejemplo. Supongamos que hay una relaci√≥n causal entre el ingreso (I) y la satisfacci√≥n (S), a m√°s ingreso m√°s satisfacci√≥n."
  },
  {
    "objectID": "intro_R.html#tidy-data",
    "href": "intro_R.html#tidy-data",
    "title": "1¬† R y el tidyverse",
    "section": "1.1 Tidy data",
    "text": "1.1 Tidy data\nLo primero que tenemos que pensar cuando trabajamos con el tidyverse es que nuestros datos est√©n en formato tidy. ¬øQu√© significa esto? Cuando un dataset est√° en formato tidy, cada columna corresponde a una variable y cada fila a una √∫nica observaci√≥n2. Veamos un ejemplo. Tenemos tres sujetos a los cuales les medimos el tiempo de respuesta en una tarea. Cada sujeto realiza dos repeticiones de esta medici√≥n, el trial 1 y el trial 2. En la tabla Tabla¬†1.1 podemos ver las dos formas de organizar esta informaci√≥n.2¬†El caso contrario ser√≠a en el que una fila contiene varios mediciones para distintos niveles de una variable. Este formato se conoce como wide.\n\n\nTabla¬†1.1: Ejemplo de tablas tidy y wide.\n\n\n\n\n(a) Tidy \n \n  \n    sujeto \n    trial \n    tiempo_respuesta \n  \n \n\n  \n    Jerry \n    1 \n    0.0807501 \n  \n  \n    Jerry \n    2 \n    0.8343330 \n  \n  \n    Elaine \n    1 \n    0.6007609 \n  \n  \n    Elaine \n    2 \n    0.1572084 \n  \n  \n    George \n    1 \n    0.0073994 \n  \n  \n    George \n    2 \n    0.4663935 \n  \n\n\n\n\n\n\n(b) Wide \n \n  \n    sujeto \n    trial_1 \n    trial_2 \n  \n \n\n  \n    Jerry \n    0.4977774 \n    0.7725215 \n  \n  \n    Elaine \n    0.2897672 \n    0.8746007 \n  \n  \n    George \n    0.7328820 \n    0.1749406 \n  \n\n\n\n\n\n\nA lo largo de este cap√≠tulo iremos viendo los beneficios de almacenar los datos en formato tidy. Por supuesto que estas ventajas tienen su precio, principalemente que las bases de datos crecen mucho en tama√±o si tenemos muchas medidas repetidas con distintos valores de las variables."
  },
  {
    "objectID": "intro_R.html#introducci√≥n-al-tidyverse",
    "href": "intro_R.html#introducci√≥n-al-tidyverse",
    "title": "1¬† R y el tidyverse",
    "section": "1.2 Introducci√≥n al Tidyverse",
    "text": "1.2 Introducci√≥n al Tidyverse\nComo contamos m√°s arriba, el tidyverse es una colecci√≥n cerca de 25 paquetes, todos relacionados con la carga, manejo, modificaci√≥n y visualizaci√≥n de datos. La idea de este libro no es profundizar en todas sus capacidades pero consideramos importante presentar algunas de las funciones que m√°s vamos a utilizar a lo largo del libro. Estas son funciones para leer datos del paquete {readr}, los verbos de {dplyr} para manipularlos, las funciones de {tidyR} para acomodarlos y el poderos√≠simo {ggplot2} para visualizarlos.\n\n1.2.1 Cargando datos con readr\nUna de las cosas que vamos a hacer m√°s a menudo en este libro es cargar alg√∫n dataset. Para esto vamos a usar varias de las funcionalidades del paquete {readr}.\nEl caso m√°s simple al que nos vamos a enfrentar es la carga de una base de datos organizada en columnas y separadas por comas en un archivo de extensi√≥n .csv. En este caso lo que tenemos que hacer es bastante simple, usar la funci√≥n read_csv() como a continuaci√≥n:\n\n\nC√≥digo\nsummer <- read_csv(\"../data/summer.csv\")\n\n\nPodemos ver que al cargar los datos read_csv nos dice que hay ocho columnas chr (o sea de texto) y una dbl (o sea, un n√∫mero). Si usamos la funci√≥n summary podemos ver un detalle de cada avriable con su tipo y alguna descripci√≥n3:3¬†Existen alternativas para visualizar r√°pidamente un conjunto de datos como str o glimpse o la funci√≥n skim del paquete {skimr}.\n\n\nC√≥digo\nsummary(summer)\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\n\nLos datos adentro de summer.csv son los ganadores de medallas en los juegos ol√≠mpicos de verano. Podemos ver algunas filas de muestra:\n\n\nC√≥digo\nhead(summer)\n#> # A tibble: 6 √ó 9\n#>    Year City   Sport    Discipline Athlete               Country Gender\n#>   <dbl> <chr>  <chr>    <chr>      <chr>                 <chr>   <chr> \n#> 1  1896 Athens Aquatics Swimming   HAJOS, Alfred         HUN     Men   \n#> 2  1896 Athens Aquatics Swimming   HERSCHMANN, Otto      AUT     Men   \n#> 3  1896 Athens Aquatics Swimming   DRIVAS, Dimitrios     GRE     Men   \n#> 4  1896 Athens Aquatics Swimming   MALOKINIS, Ioannis    GRE     Men   \n#> 5  1896 Athens Aquatics Swimming   CHASAPIS, Spiridon    GRE     Men   \n#> 6  1896 Athens Aquatics Swimming   CHOROPHAS, Efstathios GRE     Men   \n#> # ‚Ñπ 2 more variables: Event <chr>, Medal <chr>\n\n\nEl formato en el que read_csv almacena los datos se llama tibble y es el formato por excelencia del tidyverse. De momento lo √∫nico que nos importa es que es un formato que almacena los casos en filas y las variables en columnas (cada variable tiene un formato). Para m√°s informaci√≥n sobre las cualidades de este formato, les recomiendo revisar la documentaci√≥n.\n\n\n1.2.2 El operador pipe (|>) del paquete {magrittr}\nEl operador pipe nos permite concatenar funciones que utilizan como entrada los mismos datos. El principio de operaci√≥n es el siguiente, supongan que nosotros queremos cargar un dataset y aplicarle la funci√≥n summary. Esto lo podemos hacer simplemente cargando el dataset en una l√¨nea de c√≥digo y ejecutanco la funci√≥n summary() en la siguiente.\n\n\nC√≥digo\ndata <- read_csv(\"../data/summer.csv\")\nsummary(data)\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\n\nPero, tambi√©n podemos aprovechar el operador pipe y hacer todo en una √∫nica l√≠nea de c√≥digo.\n\n\nC√≥digo\nread_csv(\"../data/summer.csv\") |> summary()\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\n\nAl dejar vac√≠o el par√©ntesis de la funci√≥n summary(), la misma va a tomar como variable de entrada a la que est√° antes del operador pipe, es decir, a la que antes llamamos data. En el caso que la funci√≥n summary() tuviera m√°s de una variable de entrada, lo que viene antes del pipe tomar√≠a el lugar de la primera de ellas.\nSi bien esta funcionalidad parece algo que complica las cosas y que no trae demasiados beneficios con un ejemplo tan simple, m√°s adelante veremos que puede ser de gran utilidad, ayudando a disminuir la cantidad de l√≠nes de c√≥digo y de variables intermedias.\n\n\n1.2.3 {dplyr} y sus verbos\nUna de las cosas m√°s √∫tiles del tidyverse para el tipo de procesamiento de datos que vamos a llevar a cabo en este libro son los verbos de dplyr. Estas funciones no permiten agregar columnas, resumir la informaci√≥n, filtrar filas, seleccionar columnas, etc4. Y todas estas acciones las podemos hacer en la base de datos completa o en una parte de ella agrupada de acuerdo a alg√∫n criterio. Vayamos de a poco.4¬†Para m√°s detalles sobre los verbos disponibles en el paquete {dplyr} pueden visital este la p√°gina de referencia.\n\n1.2.3.1 El verbo filter\nVolvamos a los datos de los JJOO de verano. Supongamos que nos queremos quedar s√≥lo con las medallas de Argentina. Para este tipo de filtrado de filas (o casos, o mediciones) {dplyr} tiene un verbo que se llama filter y funciona de la siguiente forma5:5¬†Se preguntar√°n por qu√© antes de la funci√≥n filter aparece un ::dplyr. Esto es simplemente una forma de decirle a R que la funci√≥n filter que debe utilizar es la del paquete {dplyr}. Esta es una pr√°ctica recomendable sobre todo para funciones con nombres comunes como filter o select.\n\n\nC√≥digo\nsummer |> dplyr::filter(Country == \"ARG\") |> head(10)\n#> # A tibble: 10 √ó 9\n#>    Year City  Sport     Discipline Athlete          Country Gender\n#>   <dbl> <chr> <chr>     <chr>      <chr>            <chr>   <chr> \n#> 1  1924 Paris Athletics Athletics  BRUNETO, Luis    ARG     Men   \n#> 2  1924 Paris Boxing    Boxing     PORZIO, Alfredo  ARG     Men   \n#> 3  1924 Paris Boxing    Boxing     QUARTUCCI, Pedro ARG     Men   \n#> 4  1924 Paris Boxing    Boxing     COPELLO, Alfredo ARG     Men   \n#> 5  1924 Paris Boxing    Boxing     MENDEZ, Hector   ARG     Men   \n#> 6  1924 Paris Polo      Polo       KENNY, Arturo    ARG     Men   \n#> # ‚Ñπ 4 more rows\n#> # ‚Ñπ 2 more variables: Event <chr>, Medal <chr>\n\n\nNoten que estamos utilizando el operador |> para concatenar las acciones: Con los datos de summer hacemos el filtrado y, luego, mostramos las primeras diez filas de esos datos ya filtrados.\nTambi√©n podr√≠amos quere quedarnos con las medallas de Argenitna en los JJOO de Atenas 2004, para esto debemos el operador l√≥gico ‚Äúy‚Äù, cuyo s√≠mbolo en R es &:\n\n\nC√≥digo\nsummer |> dplyr::filter(Country == \"ARG\" & Year == 2004) |> head(5)\n#> # A tibble: 5 √ó 9\n#>    Year City   Sport      Discipline Athlete                   Country Gender\n#>   <dbl> <chr>  <chr>      <chr>      <chr>                     <chr>   <chr> \n#> 1  2004 Athens Aquatics   Swimming   BARDACH, Georgina         ARG     Women \n#> 2  2004 Athens Basketball Basketball DELFINO, Carlos Francisco ARG     Men   \n#> 3  2004 Athens Basketball Basketball FERNANDEZ, Gabriel Diego  ARG     Men   \n#> 4  2004 Athens Basketball Basketball GINOBILI, Emanuel David   ARG     Men   \n#> 5  2004 Athens Basketball Basketball GUTIERREZ, Leonardo Mart‚Ä¶ ARG     Men   \n#> # ‚Ñπ 2 more variables: Event <chr>, Medal <chr>\n\n\nQue linda esa Generaci√≥n DoradaüèÖ, ¬øNo?.Por otro lado, si nos queremos quedar con las medallas de Argentina o Brasil debemos utilizar el operador l√≥gico ‚Äúo‚Äù, cuyo s√≠mbolo en R es |:\n\n\nC√≥digo\nsummer |> dplyr::filter(Country == \"ARG\" | Country == \"BRA\") |> head(10)\n#> # A tibble: 10 √ó 9\n#>    Year City    Sport    Discipline Athlete                   Country Gender\n#>   <dbl> <chr>   <chr>    <chr>      <chr>                     <chr>   <chr> \n#> 1  1920 Antwerp Shooting Shooting   PARAENSE, Guilherme       BRA     Men   \n#> 2  1920 Antwerp Shooting Shooting   BARBOSA, Dario            BRA     Men   \n#> 3  1920 Antwerp Shooting Shooting   DA COSTA, Afranio Antonio BRA     Men   \n#> 4  1920 Antwerp Shooting Shooting   PARAENSE, Guilherme       BRA     Men   \n#> 5  1920 Antwerp Shooting Shooting   SOLEDADE, Fernando        BRA     Men   \n#> 6  1920 Antwerp Shooting Shooting   WOLF, Sebastiao           BRA     Men   \n#> # ‚Ñπ 4 more rows\n#> # ‚Ñπ 2 more variables: Event <chr>, Medal <chr>\n\n\nAunque, una alternativa muy √∫til cuando tenemos los valores de una variable que queremos filtrar en un array es:\n\n\nC√≥digo\nsummer |> dplyr::filter(Country %in% c(\"ARG\", \"BRA\")) |> head(10)\n#> # A tibble: 10 √ó 9\n#>    Year City    Sport    Discipline Athlete                   Country Gender\n#>   <dbl> <chr>   <chr>    <chr>      <chr>                     <chr>   <chr> \n#> 1  1920 Antwerp Shooting Shooting   PARAENSE, Guilherme       BRA     Men   \n#> 2  1920 Antwerp Shooting Shooting   BARBOSA, Dario            BRA     Men   \n#> 3  1920 Antwerp Shooting Shooting   DA COSTA, Afranio Antonio BRA     Men   \n#> 4  1920 Antwerp Shooting Shooting   PARAENSE, Guilherme       BRA     Men   \n#> 5  1920 Antwerp Shooting Shooting   SOLEDADE, Fernando        BRA     Men   \n#> 6  1920 Antwerp Shooting Shooting   WOLF, Sebastiao           BRA     Men   \n#> # ‚Ñπ 4 more rows\n#> # ‚Ñπ 2 more variables: Event <chr>, Medal <chr>\n\n\nFinalmente, si tenemos una variable num√©rica, podemos filtrar con condiciones como mayor o menor:\n\n\nC√≥digo\nsummer |> dplyr::filter(Year > 2010) |> head(5)\n#> # A tibble: 5 √ó 9\n#>    Year City   Sport    Discipline Athlete          Country Gender\n#>   <dbl> <chr>  <chr>    <chr>      <chr>            <chr>   <chr> \n#> 1  2012 London Aquatics Diving     BOUDIA, David    USA     Men   \n#> 2  2012 London Aquatics Diving     QIU, Bo          CHN     Men   \n#> 3  2012 London Aquatics Diving     DALEY, Thomas    GBR     Men   \n#> 4  2012 London Aquatics Diving     CHEN, Ruolin     CHN     Women \n#> 5  2012 London Aquatics Diving     BROBEN, Brittany AUS     Women \n#> # ‚Ñπ 2 more variables: Event <chr>, Medal <chr>\n\n\n\n\n1.2.3.2 El verbo select\nEl verbo select es similar a filter pero nos permite filtrar no casos sino variables. Por ejemplo, ¬øQu√© pasa si solo nos interesa el a√±o, la ciudad y el nombre del atleta?:\n\n\nC√≥digo\nsummer |> dplyr::select(c(Year, City, Athlete)) |> head(5)\n#> # A tibble: 5 √ó 3\n#>    Year City   Athlete           \n#>   <dbl> <chr>  <chr>             \n#> 1  1896 Athens HAJOS, Alfred     \n#> 2  1896 Athens HERSCHMANN, Otto  \n#> 3  1896 Athens DRIVAS, Dimitrios \n#> 4  1896 Athens MALOKINIS, Ioannis\n#> 5  1896 Athens CHASAPIS, Spiridon\n\n\n\n\n1.2.3.3 El verbo mutate\nAhora las cosas se complican un poco. mutate es un verbo que nos permite crear nuevas columnas ya sea con datos nuevos o en funci√≥n de los datos existentes. Por ejemplo, creemos una columna nueva que tenga un chr con el pa√≠s, un gui√≥n y el nombre del atleta y llam√©mosla nationality_athlete. Nos vamos a quedar s√≥lo con el a√±o, la medalla que gan√≥ y el nuevo nombre combinado con la nacionalidad:\n\n\nC√≥digo\nsummer |> \n  dplyr::mutate(nationality_athlete = paste(Country, \"-\", Athlete)) |> \n  dplyr::select(c(Year, Medal, nationality_athlete)) |>\n  head(5)\n#> # A tibble: 5 √ó 3\n#>    Year Medal  nationality_athlete     \n#>   <dbl> <chr>  <chr>                   \n#> 1  1896 Gold   HUN - HAJOS, Alfred     \n#> 2  1896 Silver AUT - HERSCHMANN, Otto  \n#> 3  1896 Bronze GRE - DRIVAS, Dimitrios \n#> 4  1896 Gold   GRE - MALOKINIS, Ioannis\n#> 5  1896 Silver GRE - CHASAPIS, Spiridon\n\n\nO, por ejemplo, podemos querer crear una variable que nos ponga un \\(1\\) si es griego y un \\(0\\) si no6:6¬†Para m√°s detalles sobre la funci√≥n if_else pueden ver el siguiente link.\n\n\nC√≥digo\nsummer |> \n  dplyr::mutate(is_greek = if_else(Country == \"GRE\", 1, 0)) |> \n  dplyr::select(c(Year, Medal, Country, is_greek)) |>\n  head(5)\n#> # A tibble: 5 √ó 4\n#>    Year Medal  Country is_greek\n#>   <dbl> <chr>  <chr>      <dbl>\n#> 1  1896 Gold   HUN            0\n#> 2  1896 Silver AUT            0\n#> 3  1896 Bronze GRE            1\n#> 4  1896 Gold   GRE            1\n#> 5  1896 Silver GRE            1\n\n\nAhora vamos a aprender algo muy importante y cool üÜí: A agrupar los casos de acuerdo a una variable. Por ejemplo, si queremos agregar una columna que contenga la cantidad total de medallas ganadas por un pa√≠s a cada atleta de ese pa√≠s podemos hacer lo siguiente:\n\n\nC√≥digo\nsummer |> \n  group_by(Country) |>\n  dplyr::mutate(num_medals = n()) |> \n  dplyr::select(c(Year, Medal, Athlete, num_medals)) |>\n  head(5)\n#> # A tibble: 5 √ó 5\n#> # Groups:   Country [3]\n#>   Country  Year Medal  Athlete            num_medals\n#>   <chr>   <dbl> <chr>  <chr>                   <int>\n#> 1 HUN      1896 Gold   HAJOS, Alfred            1079\n#> 2 AUT      1896 Silver HERSCHMANN, Otto          146\n#> 3 GRE      1896 Bronze DRIVAS, Dimitrios         148\n#> 4 GRE      1896 Gold   MALOKINIS, Ioannis        148\n#> 5 GRE      1896 Silver CHASAPIS, Spiridon        148\n\n\n¬øPerdidos? Tomens√© su tiepo para tratar de entender qu√© pas√≥ y prueben distintas alternativas en sus computadoras.\n\n\n1.2.3.4 El verbo summarise\nPor √∫ltimo, el verbo summarise nos permite sacar medidas resumen de nuestros datos. Empecemos con algo obvio: ¬øCu√°ntas medallas de oro gan√≥ cada pa√≠s en la historia de los juegos ol√≠mpicos?. Podemos hacer algo parecido a lo √∫ltimo que hicimos con mutate pero el resultados ser√° ligeramente diferente7:7¬†La funci√≥n arrange nos ordena los datos de acuerdo a la variable que le enviemos como par√°metro de menos a mayor. Si queremos que ordene de mayor a menor debemos agregar la funci√≥n desc en el argumento. M√°s detalles ac√°.\n\n\nC√≥digo\nsummer |> \n  dplyr::filter(Medal == \"Gold\") |>\n  group_by(Country) |>\n  dplyr::summarise(num_medals = n()) |>\n  arrange(desc(num_medals)) |>\n  head(10)\n#> # A tibble: 10 √ó 2\n#>   Country num_medals\n#>   <chr>        <int>\n#> 1 USA           2235\n#> 2 URS            838\n#> 3 GBR            546\n#> 4 ITA            476\n#> 5 GER            452\n#> 6 HUN            412\n#> # ‚Ñπ 4 more rows\n\n\nHay algo raro, ¬øNo? Bueno, s√≠, de esta forma estamos contando a todos los atletas que tuvieron la misma medalla (por ejemplo, si la medalla fue por f√∫tbol estamos contando cerca de 30 medallas). Para resolver esto nos podemos sacar de encima los casos duplicados por a√±o, deporte, disciplina, evento y g√©nero8:8¬†La funci√≥n distinct nos conserva una sola realizaci√≥n de cada caso que es igual de acuerdo a las variables que le pasemos como par√°metros. M√°s detalles ac√°.\n\n\nC√≥digo\nsummer |> \n  distinct(Year, Sport, Discipline, Event, Gender, .keep_all = TRUE) |>\n  dplyr::filter(Medal == \"Gold\") |>\n  group_by(Country) |>\n  dplyr::summarise(num_medals = n()) |>\n  arrange(desc(num_medals)) |>\n  head(5)\n#> # A tibble: 5 √ó 2\n#>   Country num_medals\n#>   <chr>        <int>\n#> 1 USA             67\n#> 2 GBR             46\n#> 3 CHN             40\n#> 4 RUS             22\n#> 5 GER             19\n\n\nVayamos con lo √∫ltimo, calculemos la media y la desviaci√≥n est√°ndar de las medallas de Argentina por JJOO combinando todo lo que vimos.\n\n\nC√≥digo\nsummer |> \n  distinct(Year, Sport, Discipline, Event, Medal, Gender, .keep_all = TRUE) |>\n  dplyr::filter(Country == \"ARG\") |>\n  group_by(Country, Year) |>\n  dplyr::summarise(num_medals = n()) |>\n  ungroup() |>\n  summarise(media  = mean(num_medals),\n            desvio = sd(num_medals))\n#> # A tibble: 1 √ó 2\n#>   media desvio\n#>   <dbl>  <dbl>\n#> 1  3.83   2.28\n\n\nDigieran esto tranquilos.\n\n\n\n1.2.4 {tidyR}, el paquete para ordenar tus datos\nEl paquete {tidyR} tiene muchas herramientas de manejo de tablas como reformatear, expandir tablas, manejar valores faltantes, dividir celdas, anidar datos, etc9. Sin embargo, en esta breve introducci√≥n s√≥lo vamos a presentar muy brevemente las herramientas que nos permiten convertir una tabla wide en tidy (o long) y viceverse.9¬†Para m√°s informaci√≥n ver el cheatsheet.\n\n1.2.4.1 La funci√≥n pivot_longer\nVolvamos a la tabla iniicial que ten√≠amos en formato wide:\n\n\nC√≥digo\ntabla_wide <- tibble(sujeto  = rep(c(\"Jerry\", \"Elaine\", \"George\")),\n                     trial_1 = runif(3),\n                     trial_2 = runif(3)) \n\ntabla_wide\n#> # A tibble: 3 √ó 3\n#>   sujeto trial_1 trial_2\n#>   <chr>    <dbl>   <dbl>\n#> 1 Jerry    0.320  0.404 \n#> 2 Elaine   0.402  0.0637\n#> 3 George   0.196  0.389\n\n\nSi nosotros quisi√±eramos transformar esta tabla en una tabla en formato tidy podemos utilizar la funci√≥n pivot_longer10. Veamos como funciona y despu√©s la desmenuzamos:10¬†M√°s informaci√≥n ac√°.\n\n\nC√≥digo\npivot_longer(data = tabla_wide, \n             cols = trial_1:trial_2, \n             names_to = \"trial\",\n             values_to = \"tiempo_respuesta\")\n#> # A tibble: 6 √ó 3\n#>   sujeto trial   tiempo_respuesta\n#>   <chr>  <chr>              <dbl>\n#> 1 Jerry  trial_1           0.320 \n#> 2 Jerry  trial_2           0.404 \n#> 3 Elaine trial_1           0.402 \n#> 4 Elaine trial_2           0.0637\n#> 5 George trial_1           0.196 \n#> 6 George trial_2           0.389\n\n\nLos argumentos son los siguientes: data es la tabla a la que le vamos a realizar el cambio de formato; cols son las columnas que vamos a cambiar, en este caso desde trial_1 a trial_2; en names_to indicamos la variable a la que vamos a mandar los nombres de las columnas actuales; y values_to la variables a la que vamos a mandar los valores.\nAlgo ligeramente raro es que la columna trial no es num√©rica y, s√≥lo por completitud, lo vamos a solucionar usando a nuestro gran amigo |> y al verbo mutate11:11¬†Y la funci√≥n parse_number del paquete {readr}.\n\n\nC√≥digo\npivot_longer(data = tabla_wide, \n             cols = trial_1:trial_2, \n             names_to = \"trial\",\n             values_to = \"tiempo_respuesta\") |>\n  mutate(trial = parse_number(trial))\n#> # A tibble: 6 √ó 3\n#>   sujeto trial tiempo_respuesta\n#>   <chr>  <dbl>            <dbl>\n#> 1 Jerry      1           0.320 \n#> 2 Jerry      2           0.404 \n#> 3 Elaine     1           0.402 \n#> 4 Elaine     2           0.0637\n#> 5 George     1           0.196 \n#> 6 George     2           0.389\n\n\n\n\n1.2.4.2 La funci√≥n pivot_wider\nAhora vamos con el caso contrario en el que tenemos una tabla en formato long y la queremos convertir en wide:\n\n\nC√≥digo\ntabla_long <- pivot_longer(data = tabla_wide, \n                           cols = trial_1:trial_2, \n                           names_to = \"trial\",\n                           values_to = \"tiempo_respuesta\") |>\n  mutate(trial = parse_number(trial))\n\ntabla_long\n#> # A tibble: 6 √ó 3\n#>   sujeto trial tiempo_respuesta\n#>   <chr>  <dbl>            <dbl>\n#> 1 Jerry      1           0.320 \n#> 2 Jerry      2           0.404 \n#> 3 Elaine     1           0.402 \n#> 4 Elaine     2           0.0637\n#> 5 George     1           0.196 \n#> 6 George     2           0.389\n\n\nPara esto vamos a hechar mano a la funci√≥n pivot_wider12 que tiene una sint√°xis parecida a su prima pivot_longer:12¬†M√°s informaci√≥n ac√°.\n\n\nC√≥digo\npivot_wider(data = tabla_long, \n            names_from = trial, \n            values_from = tiempo_respuesta)\n#> # A tibble: 3 √ó 3\n#>   sujeto   `1`    `2`\n#>   <chr>  <dbl>  <dbl>\n#> 1 Jerry  0.320 0.404 \n#> 2 Elaine 0.402 0.0637\n#> 3 George 0.196 0.389\n\n\nEt Voil√†!, ya tenemos nuestra tabla en formato wide. En este caso le dijimos de que variable tomar los nombres de las nuevas columnas en names_from y de que variable tomar los valores en values_from.\nFinalmente, y s√≥lo para alimnetar nuestra obsesi√≥n, vamos a corregir los nombres de las columnas agregando el prefijo trial_ utilizando el par√°metro de la funci√≥n names_prefix:\n\n\nC√≥digo\npivot_wider(data = tabla_long, \n            names_from = trial, \n            names_prefix = \"trial_\",\n            values_from = tiempo_respuesta)\n#> # A tibble: 3 √ó 3\n#>   sujeto trial_1 trial_2\n#>   <chr>    <dbl>   <dbl>\n#> 1 Jerry    0.320  0.404 \n#> 2 Elaine   0.402  0.0637\n#> 3 George   0.196  0.389"
  },
  {
    "objectID": "intro_R.html#algunas-palabras-finales",
    "href": "intro_R.html#algunas-palabras-finales",
    "title": "1¬† R y el tidyverse",
    "section": "1.3 Algunas palabras finales",
    "text": "1.3 Algunas palabras finales\nComo vimos brevemente en este cap√≠tulo, los paquetes del tidyverse son una herramineta important√≠sima para el an√°lisis de datos utilizando R. Para m√°s detalles sobre estas funcionalidades les recomendamos la gu√≠a de Hadley Wickham(Wickham et¬†al. 2019) o, si ya se quieren sumergir de lleno en el mundo del an√°lisis de datos con R, este fant√°stico libro [Wickham, √áetinkaya-Rundel, y Grolemund (2023)]13. Es decir, sin tener que cargar ning√∫n paquete de funciones adicional..13¬†Disponible gratis online en ac√°.\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et¬†al. 2019. ¬´Welcome to the Tidyverse¬ª. Journal of open source software 4 (43): 1686.\n\nWickham, Hadley, Mine √áetinkaya-Rundel, y Garrett Grolemund. 2023. R for data science. \" O‚ÄôReilly Media, Inc.\"."
  },
  {
    "objectID": "potential_outcomes.html#presentaci√≥n",
    "href": "potential_outcomes.html#presentaci√≥n",
    "title": "3¬† Potential outcomes",
    "section": "3.1 Presentaci√≥n",
    "text": "3.1 Presentaci√≥n\nSupongamos que quiero evaluar la efectividad de la aspirina para mitigar el dolor de cabeza. Me duele la cabeza y lo quiero es saber el efecto diferencial entre tomar y no tomar esa aspirina. Es decir, en el tiempo 0 estoy yo con dolor de cabeza y en el tiempo 1 deber√≠a haber dos versiones m√≠as (como si una no fuera suficiente), la que tom√≥ la aspirina y la que no. A cada una de ellas les tendr√≠a que preguntar cu√°nto les duele la cabeza, el outcome de mi comparaci√≥n. No hace falta ser demasiado astuto para darse cuenta que esto es imposible ya que s√≥lo nos ser√° posible obsevar una de esas versiones mientras que la otra ser√° un contraf√°ctico.\nDe esto vamos a hablar en este cap√≠tulo, utilizando la tradici√≥n de los potential outcomes. Estas ideas terminan de tomar forma en la versi√≥n que conocemos en las ciencias sociales en (Rubin 1974)"
  },
  {
    "objectID": "intro_stat.html#inferencia-estad√≠stica",
    "href": "intro_stat.html#inferencia-estad√≠stica",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.10 Inferencia estad√≠stica",
    "text": "2.10 Inferencia estad√≠stica\nVamos a hacer un breve paseo por los conceptos clave de la inferencia estad√≠stica de la mano de un ejemplo.\n\n\n\n\n\n\nEl ejemplo de inferencia\n\n\n\nSupongamos que tenemos una p√°gina web de noticias y queremos probar una nueva feature con la que queremos aumentar el tiempo de retenci√≥n de los usuarios. Para esto le vamos a presentar a los usuarios la versi√≥n nueva de la p√°gina y vamos a medir el tiempo que se mantienen en la p√°gina en ms6.6¬†Probabilemente lo m√°s correcto para responder esta pregunta sea un A/B test, pero ya hablaremos de eso m√°s adelante.\n\n\nPrimero definamos nuestra variable aleatoria: \\(X\\):‚ÄúLa diferencia de tiempo en ms entre despu√©s y antes del cambio‚Äù. Nuestro objetivo entonces es poder afirmar con cierto grado de seguridad si \\(E(X)\\) es igual a cero o distinto (nos importa tanto si aumenta como si disminuye). Empecemos con lo m√°s sencillo, supongamos que \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\). Esta suposici√≥n no es tan loca ya que mucho procesos naturales se distribuyen de forma normal7. Supongamos tambi√©n por un momento (m√°s adelante vamos a relajar esta condici√≥n) que, ya sea por un experimento previo o porque somos magos, conocemos la \\(\\sigma^2\\) de \\(X\\).7¬†Adem√°s, como vamos a ver m√°s adelante, cuando el \\(n\\) es lo suficientemente grande, esta condici√≥n deja de importar tanto.\nComo dijimos en el recuedro, si bien nuestro objetivo es probar si \\(\\mu\\) es diferente de \\(0\\), esto lo vamos a hacer a partir de un experimento. Una vez que hagamos el experimento y midamos vamos a tener al cl√°sico estimador de \\(\\mu\\): \\(\\hat{\\mu}=\\bar{X}\\), es decir, el promedio muestral. Ahora supongamos que el promedio nos da \\(0.5\\): ¬øEs distinto de cero? Para responder esa pregunta es que vamos a utilizar las herramientas de la inferencia estad√≠stica.\nDefinamos primero las hip√≥tesis:\n\\[\n\\begin{array}\n_H_0 &:& \\mu = 0 \\\\\nH_1 &:& \\mu \\neq 0\n\\end{array}\n\\tag{2.19}\\]\nLo que vamos a querer hacer es rechazar \\(H_0\\) con cierto grado de confianza. Para esto vamos a comparar nuestra medici√≥n \\(\\bar{x}_{obs}\\) con la distribuci√≥n de los \\(\\bar{X}\\) bajo \\(H_0\\).\nRecordemos que si \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) entonces \\(\\bar{X}_n \\sim \\mathcal{N}(\\mu, \\sigma^2/n)\\)8, donde \\(n\\) es la cantidad de realizaciones con las que yo calculo mi \\(\\bar{X}\\). Entonces si mi \\(\\bar{x}_{obs}\\) medida est√° lo suficientemente lejos de \\(0\\) podemos decir que que \\(\\mu\\) es diferente de cero. Pero: ¬øQu√© es suficientemente lejos?8¬†La \\(n\\) en \\(\\bar{X}_n\\) es simplemente para enfatizar que es el promedio de una seciencia de \\(n\\) realizaciones.\nBueno, para responder esa pregunta vamos a tener que primero definir los errores que podemos cometer. Podemos cometer el error de decir que \\(\\mu\\) es diferente de cero cuando no lo es (Error tipo I o dalso positivo) o podemos cometer el error de decir que \\(\\mu\\) no es diferente de cero cuando s√≠ lo es (Error tipo II o dalso negativo). Nuestro razonamiento va a ser empezar acotando el error de tipo I.\nPara esto vamos a calcular qu√© tal probable es observar un valor igual o m√°s alejado del cero que \\(\\bar{x}_{obs}\\) dado que \\(H_0\\) es verdadera. Es decir, \\(P(|\\bar{X}|>\\bar{x}_{obs}|H_0)\\). Esta magnitud es lo que se conoce como el viejo y querido p-valor o p-value (si te gusta hacerte el canchero). En nuestro caso lo podemos calcular expl√≠citamente. Empecemos estandarizando \\(\\bar{X}\\) de la siguiente forma:\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sqrt{\\sigma^2/n}} \\sim \\mathcal{N}(0,1)\n\\tag{2.20}\\]\nQque bajo \\(H_0\\) es:\n\\[\nZ_{H_0} = \\frac{\\bar{X}}{\\sqrt{\\sigma^2/n}} \\sim \\mathcal{N}(0,1)\n\\tag{2.21}\\]\nEntonces, si consideramos a \\(z_{\\bar{x}}\\) como la versi√≥n estandarizada de \\(\\bar{x}_{obs}\\), podemos calcular el p-valor como:\n\\[\n\\begin{array}\n_p_{valor} &=& P(|Z| \\geq |z_{\\bar{x}}| \\: \\: |H_0) \\\\\n&=& P(Z \\geq |z_{\\bar{x}}| \\: \\: |H_0) + P(Z \\leq -|z_{\\bar{x}}| \\: \\: |H_0) \\\\\n&=& 2 P(Z \\leq -|z_{\\bar{x}}| \\: \\: |H_0)\n\\end{array}\n\\tag{2.22}\\]\nY como bajo \\(H_0\\) ocurre que \\(Z_{H_0} \\sim \\mathcal{N}(0,1)\\):\n\\[\np_{valor} = 2 \\phi(-|z_{\\bar{x}}|)\n\\tag{2.23}\\]\nY ahora que tenemos esta probabilidad qu√© hacemos. Bueno, lo que podemos hacer es decir: Si los datos vienen de \\(H_0\\) podemos calcular que tan ‚Äúraros‚Äù son, entonces pongamos una cota en esa probabilidad y de esa forma estaremos acotando el error de tipo I en el largo plazo9. As√≠ es que surge el famoso \\(\\alpha\\) que normalmente hacemos valer \\(0.05\\)10. ¬øQu√© significa eso? Bueno, significa que, si la hip√≥tesis nula fuera verdaera dir√≠amos euivocadamente que hay un efecto cuando no lo hay el \\(5%\\) de las veces. Este p√°rrafo se podr√≠a extender al infinito, pero por ahora qued√©monos con esta interpretaci√≥n ‚Äúpr√°ctica‚Äù del p-valor (si se quedan con las ganas pueden ir a leer esto).9¬†El enfoque frecuentista de la estad√≠stica justamente se basa en controlar los errores dada una repetici√≥n infinita de mi experimento.10¬†Para una discusi√≥n m√°s profunda sobre el tema de la selecci√≥n de \\(\\alpha\\) en la psicolog√≠a experimental les recomiendo este hermoso art√≠culo de Maier y Lakens (Maier y Lakens 2022).\nMaier, Maximilian, y Dani√´l Lakens. 2022. ¬´Justify your alpha: A primer on two practical approaches¬ª. Advances in Methods and Practices in Psychological Science 5 (2): 25152459221080396.\n\nEntonces el camino es el siguiente: Tomamos las medidas, calculamos el promedio, calculamos el p-valor y si este es menor que \\(\\alpha\\) podemos decir que \\(H_0\\) es falsa. Todo muy lindo, pero siempre tengamos en mente que no sabemos exactamente si para esa realizaci√≥n estamos comentiendo un error de tipo I o no, y ese es uno de las limitaciones de la estad√≠stica frecuentista.\nSimulemos un experimento para \\(n=50\\) en el que nosotros conocemos tanto \\(\\sigma^2\\) como \\(\\mu\\):\n\n\nVer el c√≥digo\nset.seed(123)\nn <- 50\nmu <- .5\nsigma <- 2\n\nX <- rnorm(n, mu, sigma)\n\ncat(paste(\"El promedio muestral es:\", round(mean(X), 3)))\n#> El promedio muestral es: 0.569\ncat(paste(\"El promedio muestral estandarizado es:\", round(mean(X)/sqrt(sigma^2/n), 3)))\n#> El promedio muestral estandarizado es: 2.011\n\n\nAc√° vemos que, efectivamente, \\(H_0\\) es falsa ya que \\(\\mu=0.5\\) (el verdadero par√°metro poblacional). La media observada vale 0.569 y la media estandarizada (\\(z_{\\bar{x}}\\)) vale 2.011. Miremos ccomo queda \\(z_{\\bar{x}}\\) dentro de la distribuci√≥n de los \\(Z_{H_0}\\) (los \\(\\bar{X}\\) bajo \\(H_0\\) estandarizados):\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ‚Ñπ Please use `linewidth` instead.\n\n\n\n\nFunci√≥n de densidad de probabilidad de la variabla aleatoria H (altura de los varones)\n\n\n\n\nLa curva azul es la funci√≥n de densidad de \\(Z_{H_0}\\), las l√≠neas verticales naranjas delimitan los valores de \\(Z\\) cuales la probabilidad de encontrar valores m√°s extremos es mayor a \\(0.05\\), es decir, a la derecha de la l√≠nea naranja positiva y a la izquierda de la negativa estamos en la regi√≥n en la que vamos a considerar a \\(z_{\\bar{x}}\\) como evidencia significativa de que \\(H_0\\) es falsa. La suma de las integrales de la curva azul a la derecha de la l√≠nea naranja posiva y a la izquierda de la negativa da como resultado \\(\\alpha\\).\nEl punto verde (y la l√≠nea punteada que lo acompa√±a) es nuestra observaci√≥n \\(z_{\\bar{x}}\\). O sea, todo parece indicar que controlando nuestro error de tipo I con \\(\\alpha=0.05\\) el valor observado nos permitir√≠a que podemos rechazar \\(H_0\\) o, como se dice habitualmente: ‚ÄúQue \\(\\mu\\) es significativamente diferente de cero‚Äù. Calculemos el p-valor y veamos si esto es efectivamente as√≠.\n\\[\np_{valor} = 2 \\phi(-|z_{\\bar{x}}|) = 2 \\phi(-2.011)\n\\tag{2.24}\\]\nQue en R lo podemos calcular como:\n\n\nVer el c√≥digo\ncat(paste(\"El p-valor es:\", round(2*pnorm(-mean(X)/sqrt(sigma^2/n)),3)))\n#> El p-valor es: 0.044\n\n\nQue como es menor que \\(0.05\\) nos permite rechazar \\(H_0\\).\nPara resolver este ejemplo hicimos dos consideraciones que les pueden hacer ruido: 1. Asumimos que conoc√≠amos la desviaci√≥n est√°ndar de X. 2. Asumimos que X tiene distribuci√≥n normal.\n\n2.10.1 ¬øQu√© pasa si no conozco \\(\\sigma\\)?\nCon respecto a 1, es natural que les haga ruido ya que en la mayor√≠a de los casos no conocemos al \\(\\sigma\\) poblacional sino que lo vamos a estimar. Y ¬øC√≥mo lo vamos a estimar? Echando mano del estimador insesgado de la desviaci√≥n est√°ndar \\(S\\). El mismo se define como:\n\\[\nS^2 = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}\n\\tag{2.25}\\]\nPara nuestro ejemplo vale:\n\n\nVer el c√≥digo\ncat(paste(\"la estimaci√≥n de sigma es:\", round(sd(X), 3)))\n#> la estimaci√≥n de sigma es: 1.852\n\n\nBastante cercana al valor poblacional \\(2\\).\nAhora, no es cuesti√≥n de normalizar con este nuevo \\(S^2\\) y seguir como si nada. La cosa cambia y la distribuci√≥n de \\(\\bar(X)\\) estandarizado bajo \\(H_0\\) ya no tiene una distribuci√≥n normal sino una distribuci√≥n t de student con \\(n-1\\) grados de libertad. O sea:\n\\[\n\\frac{\\bar{X} - \\mu}{\\sqrt{S^2/n}} \\sim t_{n-1}\n\\tag{2.26}\\]\nEsto se deduce a partir de que: \\[\n\\frac{\\bar{X} - \\mu}{\\sqrt{\\sigma^2/n}} \\sim \\mathcal{N}(0,1)\n\\] Y:\n\\[\n(n-1)\\frac{S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\tag{2.27}\\]\nY la definici√≥n de \\(t_{n-1}\\) es11:11¬†Dentro de la funci√≥n pt ahora se agrega el par√°metro df que representa los grados de libertad.\n\\[\n\\begin{array}\n_U &\\sim& \\mathcal{N}(0,1) \\\\\nV &\\sim& \\chi^2_{n} \\\\\n\\frac{U}{\\sqrt{V/n}} &\\sim& t_{n}\n\\end{array}\n\\tag{2.28}\\]\nCalculemos el p-valor de nuestro ejemplo, pero ahora como si no conoci√©ramos la \\(\\sigma\\) poblacional12:12¬†Dentro de la funci√≥n pt ahora se agrega el par√°metro df que representa los grados de libertad.\n\n\nVer el c√≥digo\ncat(paste(\"El p-valor es:\", round(2*pt(-mean(X)/sqrt(sd(X)^2/n), df = n-1),4)))\n#> El p-valor es: 0.0347\n\n\nUna buena noticia es que este p valor se puede calcular directamente con la funci√≥n t.test(X) de la siguiente forma:\n\n\nVer el c√≥digo\nt.test(X)\n#> \n#>  One Sample t-test\n#> \n#> data:  X\n#> t = 2.1721, df = 49, p-value = 0.03472\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  0.04254842 1.09506577\n#> sample estimates:\n#> mean of x \n#> 0.5688071\n\n\nPara cerrar, vale la pena mencionar que los par√°metros \\(\\beta\\) de un modelo lineal tambi√©n tienen distribuci√≥n t (los grados de libertad son m√°s complejos). O sea que todo lo que vimos hasta ac√° de errores tipo I, tipo II, p-valor, etc. vale tambi√©n para ellos.\n\n\n2.10.2 ¬øQu√© pasa si \\(X\\) no se distribuye normalmente?\nAhora que ya vimos que estamos haciendo cuando hacemos inferencia sobre la media, nos damos cuenta que m√°s que interesarnos la distribuci√≥n de los datos \\(X\\) nos interesa la de sus medias \\(\\bar(X)\\) (si estiman alg√∫n par√°metro de inter√©s, claro). Y ac√° viene al rescate el teorema central del l√≠mite:\n\\[\n\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}  \\xrightarrow{\\mathcal{D}} \\mathcal{N}(0,1)\n\\tag{2.29}\\]\nEl mismo nos dice que distribuci√≥n de la media estandarizada converge endistribuci√≥n a una \\(\\mathcal{N}(0,1)\\) sin importar la distribuci√≥n de \\(X\\). Esto significa que si el \\(n\\) es lo suficientemente grande podemos estar tranquilos de que no es una asunci√≥n tan loca13.13¬†Cuando las cosas no se pueden aproximar as√≠ hay soluci√≥n, existen otros tests o simplemente tests que no asumen ninguna distribuci√≥n (no param√©tricos).\nSupongamos que hay una variable aleatoria \\(V \\sim \\mathcal{E}(\\lambda=1)\\) tal que \\(E(V) = \\lambda\\). Si tomamos una muestra de \\(n=100\\) de \\(V\\), el histograma de la misma se ve algo as√≠:\n\n\n\n\n\nHistograma de una muestra de 100 datos de una exponencial con lambda=1\n\n\n\n\nDonde en azul vemos el histograma y en negro la funci√≥n de densidad para \\(\\mathcal{E}(\\lambda=1)\\). Ahora, qu√© pasa si tomamos \\(1000\\) muestras y hacemos el histograma de sus medias:\n\n\nVer el c√≥digo\nn = 1000\nZs <- c()\nset.seed(123)\nfor (i in 1:10000) {\n  x <- rexp(n, rate = 1)\n  Zs <- c(Zs, (mean(x) - 1)/sqrt(sd(x)^2/n))\n}\n\nZ_means <- tibble(Zs)\nZ_means %>% ggplot() +\n  geom_histogram(aes(x = Zs, y = ..density..), fill = \"steelblue\") +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), \n                color = \"black\", linewidth = 1) +\n  labs(title = paste(\"n = \", n), x = \"Medias de v\", y = \"Densidad\") +\n  scale_x_continuous(limits = c(-4, 4))\n#> Warning: Removed 2 rows containing non-finite outside the scale range\n#> (`stat_bin()`).\n#> Warning: Removed 2 rows containing missing values or values outside the scale range\n#> (`geom_bar()`).\n\n\n\n\n\nHistograma de 1000 medias de muestras de 100 datos de una exponencial con lambda=1\n\n\n\n\nDonde ahora la curva negra es una \\(\\mathcal{N}(0,1)\\). Hagan la prueba con otras distribuciones u otros \\(n\\) y van a ver que r√°pido (o lento para las distribuciones altamente asim√©tricas) que convergen a una \\(\\mathcal{N}(0,1)\\)."
  },
  {
    "objectID": "intro_stat.html#el-p-valor",
    "href": "intro_stat.html#el-p-valor",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.11 El p-valor",
    "text": "2.11 El p-valor\nEn construcci√≥n üöß"
  },
  {
    "objectID": "intro_stat.html#potencia-estad√≠stica",
    "href": "intro_stat.html#potencia-estad√≠stica",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.11 Potencia estad√≠stica",
    "text": "2.11 Potencia estad√≠stica\nYa hablamos de los errores de tipo I y prometimos hablar de los errores de tipo II. ¬øQu√© ser√≠a eso? Bueno, ser√≠a el caso en el que \\(H_0\\) fuera falsa y nosotros no la rechaz√°ramos. A diferencia del contros de errores de tipo I, la probabilidad de cometer errores de tipo II (llamada muy originalmente \\(\\beta\\)) depende del valor real de mi par√°metro. En el ejemplo anterior, cuando \\(H_0\\) era verdadera \\(\\mu\\) era igual a cero pero, ¬øQu√© pasa cuando es falsa? ¬øQu√© valor de \\(\\mu\\) tenemos que asumir?.\nEmpecemos definiendo a la potencia estad√≠stica, la misma se define como \\(1-\\beta\\), es decir, cuanto m√°s acotado este el error de tipo II m√°s alta ser√° la potencia. Una definici√≥n formal podr√≠a ser:\n\\[\npotencia = P(rechazar \\, H_0 | H_0 \\, falsa)\n\\tag{2.30}\\]\nQue, para el ejemplo anterior la podr√≠amos reescribir como:\n\\[\n\\begin{array}\n_potencia &=& P(rechazar \\, H_0 | H_1) \\\\\n&=& P\\left( \\left| \\frac{\\bar{X}}{\\sqrt{\\sigma^2/n}} \\right| \\geq Z_{1-\\alpha/2}  \\right)\\\\\n&=& 1 - P\\left( Z \\leq Z_{\\alpha/2} + \\frac{\\mu}{\\sqrt{\\sigma^2/n}} \\right) - P\\left( Z \\leq Z_{1-\\alpha/2} + \\frac{\\mu}{\\sqrt{\\sigma^2/n}} \\right) \\\\\n\\end{array}\n\\]\nFijens√© que para calcularla no s√≥lo necesitamos a \\(\\sigma\\) (que podr√≠amos estimar) sino tambi√©n a \\(mu\\), que es nuestro par√°metro de inter√©s. Por ejemplo, podemos ver que la pontencia depende de \\(\\mu\\), siendo m√°s grande para valores de \\(\\mu\\) m√°s alejados del cero. Esto tiene sentido ya que a medida que \\(|\\mu|\\) es mayor, es menor probable cometer errores tipo II.\n\n\nVer el c√≥digo\npotencia <- function(sigma, mu, n, alpha) {\n  pnorm(qnorm(alpha/2)-mu/(sqrt(sigma^2/n))) + 1 -\n    pnorm(qnorm(1-alpha/2)-mu/(sqrt(sigma^2/n)))\n}\n\npot <- potencia(3,seq(-3,3,.1),20,0.05)\npot_tbl <- tibble(mu = seq(-3,3,.1), potencia = pot)\npot_tbl %>% ggplot(aes(x = mu,\n           y = potencia)) +\n  geom_line(linewidth = 1) +\n  theme_bw()\n\n\n\n\n\nPotencia estad√≠stica en funci√≥n de mu.\n\n\n\n\nComo es de esperarse, la potencia tambi√©n depende del \\(\\alpha\\):\n\n\nVer el c√≥digo\npot <- potencia(3,1,20,seq(0, 0.05, 0.001))\npot_tbl <- tibble(alpha = seq(0, 0.05, 0.001), potencia = pot)\npot_tbl %>% ggplot(aes(x = alpha,\n                       y = potencia)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Potencia en funci√≥n de alfa\") +\n  theme_bw()\n\n\n\n\n\nPotencia estad√≠stica en funci√≥n de alfa.\n\n\n\n\nCon valores de potencia mayores para valores m√°s garndes de \\(\\alpha\\). Nuevamente esto tiene sentido ya que ser m√°s restictivo con el rechazo de \\(H_0\\) (o sea, que tenga que observar un valor m√°s extremo) lleva a una disminuci√≥n de \\(\\alpha\\), a un aumento de los errores tipo II y con ello a una disminuci√≥n de la potencia.\nFinalmente, la potencia tambi√©n depende del \\(n\\):\n\n\nVer el c√≥digo\npot <- potencia(3,1,seq(5, 200),0.05)\npot_tbl <- tibble(n = seq(5, 200), potencia = pot)\npot_tbl %>% ggplot(aes(x = n,\n                       y = potencia)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Potencia en funci√≥n de n\") +\n  theme_bw()\n\n\n\n\n\nPotencia estad√≠stica en funci√≥n de n.\n\n\n\n\nCon valores de potencia m√°s altos para \\(n\\) m√°s grande. Es decir, si tengo una muestra m√°s grande voy a cometer menos errores14. Esta √∫ltima dependencia es muy importante.14¬†Las distribuciones de \\(H_0\\) y \\(H_1\\) se hacen m√°s finas y hay menos solapamiento, recuerden que en ambas la varianza disminuye con \\(1/n\\).\nHay una interpretaci√≥n que me gusta que es la siguiente, la potencia estad√≠stica es la lupa con la que miramos el problema. Es decir, si tenemos una potencia alta vamos a poder detectar cambios peque√±os sin cometer demasiados errores. Supongamos que queremos dise√±ar un experimento con una dada potencia. El \\(\\alpha\\) lo decidimos cuando acotamos el error de tipo I, a \\(\\mu\\) no lo conocemos (algo vamos a hacer), entonces, lo que m√°s a mano nos queda para tener un experimento m√°s potente es aumentar el \\(n\\).\nEl uso que se le da normalmente a esta herramienta es para determinar el m√≠nimo tama√±o de muestra necesario para un experimento. El procedimiento es el siguiente:\n\nEstimamos la variabilidad de nuestro experimento de alguna forma. Lo m√°s usual es hacer un piloto pero tambi√©n puede ser un dato que salga de la bibliograf√≠a, o de experimentos anteriores que hayamos realizado.\nDeterminamos el m√≠nimo tama√±o de efecto de inter√©s (SESOI15). Esta no es una determinaci√≥n estad√≠stica sino que de dominio, tenemos que conocer el problema y pensar en cu√°l ser√≠a el m√≠nimo tama√±a de efecto que considerar√≠a relevante (relevante, no significativo). Esto a veces puede ser un poco confuso pero tambi√©n nos obliga a pensar qu√© consideramos relevante en nuestro experimento.\nUna vez que tenemos estas dos magnitudes calculamos el tama√±o de muestra para una potencia dada (por ejemplo \\(0.9\\)) y un \\(\\alpha\\) dado (por ejemplo \\(0.05\\)).\n\n15¬†Del ingl√©s smallest effect size of interest.No siempre resulta tan directo como en nuestro ejemplo, en el que podemos despejar expl√≠citamente \\(n\\), pero la forma de pensar el problema es siempre similar.\nPara m√°s detalles sobre los procedimientos para justificar el tema√±o de muestra ver (Lakens 2022).\n\n\n\nLakens, Daniel. 2022. ¬´Sample size justification¬ª. Collabra: psychology 8 (1): 33267."
  },
  {
    "objectID": "potential_outcomes.html#potential-outcomes",
    "href": "potential_outcomes.html#potential-outcomes",
    "title": "3¬† Potential outcomes",
    "section": "3.2 Potential outcomes",
    "text": "3.2 Potential outcomes\nLo que nos proponen los potential outcomes es la definici√≥n del efecto causal como la comparaci√≥n de dos estados en el mundo. En una versi√≥n del mundo, la ‚Äúactual‚Äù, me tomo una aspirina y a las dos horas registro la severidad de mi dolor de cabeza mientras que en la otra versi√≥n del mundo, la ‚Äúcontrafactual‚Äù, no me la tomo y las dos horas registro la severidad del dolor. A partir de esto, la tradici√≥n de los potential outcomes define al efecto causal de tomar una aspirina en el dolor de cabeza como la diferencia entre esas dos mediciones.\nTodo muy lindo, pero como ya estar√°n sospechando es imposible calcular un efecto que est√° expresado en funci√≥n de un contrafactual, ya que este contrafactual no lo podemos observar. Pero no se preocupen que le vamos a encontrar la vuelta.\nEmpecemos con un poco de notaci√≥n que nos va a ayudar a acomodar las ideas. Por simplicidad vamos a asumir una variable binaria para la asignaci√≥n del grupo (por ejemplo, tratamiento y control). Esta variable vale \\(1\\) si la unidad i recibe el tratamiento y \\(0\\) si no. Cada unidad \\(i\\) va a tener dos potential outcomes: \\(Y_i^1\\) si la unidad recibi√≥ el tratamiento y \\(Y_i^0\\) si no. Esto significa que una unidad experimental en el mismo momento del tiempo va a recibir y no recibir el tratamiento, o sea, alguno de estos va a ser contrafactual1.1¬†De ah√≠ el nombre de potential, porque se trata de posibles estados del mundo. Un estado en el que la unidad \\(i\\) recibe el tratamiento y uno en el que no.\nLos outcomes observables difieren de los potenciales. Mientras que los potenciales son variables aleatorias hipot√©ticas, los observables son variables aleatorias factuales y medibles. Hay una ecuaci√≥n que nos permite definir el outcome observable (\\(Y^i\\)) en funci√≥n de los potenciales, se llama la switching equation:\n\\[\nY_i = D_i Y_i^1 + (1-D_i) Y_i^0\n\\tag{3.1}\\]\nDonde \\(D_i\\) vale \\(1\\) si la unidad i recibi√≥ el tratamiento (entonces \\(Y_i=Y_i^1\\)) y \\(0\\) si no (entonces \\(Y_i=Y_i^0\\)). Vale la pena notar que \\(Y_i\\), el outcome observable, no tiene ning√∫n supra√≠ndice ya que no es m√°s potencial.\nUsando esta notaci√≥n definimos el efecto causal del tratamiento para una unidad \\(i\\) como: \\[\n\\delta_i = Y_i^1 - Y_i^0\n\\tag{3.2}\\]\nDonde queda claro que para estimar el efecto causal de acuerdo a la tradici√≥n de los potential outcomes debemos conocer dos estados del mundo a los que es imposible acceder simult√°neamente. Y aqui yace el problema funcamental de la inferencia causal: Para calcular el efecto causal se requiere acceso a datos que siempre nos van a faltar (los contraf√°cticos)(Rubin 1974).\n\nRubin, Donald B. 1974. ¬´Estimating causal effects of treatments in randomized and nonrandomized studies.¬ª Journal of educational Psychology 66 (5): 688."
  },
  {
    "objectID": "potential_outcomes.html#efecto-promedio-del-tratamiento",
    "href": "potential_outcomes.html#efecto-promedio-del-tratamiento",
    "title": "3¬† Potential outcomes",
    "section": "3.3 Efecto promedio del tratamiento",
    "text": "3.3 Efecto promedio del tratamiento\nAl igual que los potential outcomes, el efecto para la unidad \\(i\\) (\\(\\delta_i\\)) tambi√©n es una variable aleatoria, y su esperanza es lo que vamos a llamar el efecto promedio del tratamiento (ATE2). El ATE va a ser la magnitud de inter√©s en nuestros experimentos, el efecto promedio de mi tratamiento. El mismo se define de la siguiente forma:2¬†Del ingl√©s Average treatment effect.\n\\[\n\\begin{array}\n_ATE &=& E[\\delta_i] \\\\\n&=& E[Y_i^1 - Y_i^0] \\\\\n&=& E[Y_i^1] - E[Y_i^0]\n\\end{array}\n\\tag{3.3}\\]\nAhora vamos a definir el efecto promedio, pero para el grupo tratado (es decir, los participantes asignados al grupo tratamiento, con \\(D_i=1\\)):\n\\[\n\\begin{array}\n_ATT &=& E[\\delta_i|D_i=1] \\\\\n&=& E[Y_i^1 - Y_i^0|D_i=1] \\\\\n&=& E[Y_i^1|D_i=1] - E[Y_i^0|D_i=1]\n\\end{array}\n\\tag{3.4}\\]\nEsta magnitud se llama ATT3 y se calcula de la misma forma que el ATE pero condicionando los \\(\\delta_i\\) al valor de \\(D_i\\) igual a 1. De manera an√°loga definimos el efecto promedio pero para el grupo no tratado4(\\(D_i=0\\))3¬†Del ingl√©s Average treatment effect for the treated.4¬†Del ingl√©s Average treatment effect for the untreated.\n\\[\n\\begin{array}\n_ATU &=& E[\\delta_i|D_i=0] \\\\\n&=& E[Y_i^1 - Y_i^0|D_i=0] \\\\\n&=& E[Y_i^1|D_i=0] - E[Y_i^0|D_i=0]\n\\end{array}\n\\tag{3.5}\\]\nOjo con confundir estos tres conceptos. Creo que el ATE es autoexplicativo, pero se suele confundir ATT y ATU. En el primer caso, estamos calculando la esperanza de los \\(\\delta_i\\) para los individuos pertenecientes al grupo tratamiento. Esto involucra tanto sus \\(Y^1_i\\) como sus \\(Y^0_i\\). Es una confusi√≥n com√∫n confundir estos efectos promedios con magnitudes no potenciales pero, como se observa de sus f√≥rmulas, tanto estos √∫ltimos dos como el ATE no se pueden calcular en la pr√°ctica. En las secciones siguientes vamos a ver como, cumpliendo ciertas condiciones5, podemos estimar el ATE a partir de los outcomes observables.5¬†Spoiler: Asignaci√≥n aleatoria de las unidades experimentales a los grupos."
  },
  {
    "objectID": "potential_outcomes.html#diferencia-de-medias-simple",
    "href": "potential_outcomes.html#diferencia-de-medias-simple",
    "title": "3¬† Potential outcomes",
    "section": "3.4 Diferencia de medias simple",
    "text": "3.4 Diferencia de medias simple\n¬øQu√© es lo que s√≠ podemos observar? Una magnitud que a priori podr√≠amos creer que va aestar relacionada con el ATE y que podemos observar es la diferencia de medias entre los outcomes observados del grupo tratamiento y el grupo control. La vamos a llamar SDO6 y se calcula de la siguiente forma:6¬†Del ingl√©s simple difference in outcomes.\n\\[\n\\begin{array}\n_SDO &=& E[Y_i^1|D_i=1] - E[Y_i^0|D_i=0] \\\\\n&=& \\frac{1}{N_T} \\sum_{i=1}^{N_T} (y_i|d_i=1) - \\frac{1}{N_C} \\sum_{i=1}^{N_C} (y_i|d_i=0)\n\\end{array}\n\\tag{3.6}\\]\nDonde \\(N_T\\) y \\(N_C\\) son la cantidad de individuos en el grupo tratamiento y control respectivamente (y \\(N_T + N_C = n\\)). Todo muy lindo, pero operemos un poquito para ver hasta que punto el SDO es un estimador insesgado del ATE. Empecemos escribiendo el ATE como una suma pesada del ATT y el ATU:\n\\[\n\\begin{array}\n_ATE &=& \\pi ATT + (1-\\pi) ATU \\\\\n&=& \\pi E[Y_i^1|D_i=1] - \\pi E[Y_i^0|D_i=1] + \\\\\n& & (1-\\pi) E[Y_i^1|D_i=0] - (1-\\pi) E[Y_i^0|D_i=0] \\\\\n&=& \\bigl\\{ \\pi E[Y_i^1|D_i=1] + (1-\\pi) E[Y_i^1|D_i=0] \\bigl\\} - \\\\\n& & \\bigl\\{ \\pi E[Y_i^0|D_i=1] + (1-\\pi) E[Y_i^0|D_i=0] \\bigl\\}\n\\end{array}\n\\tag{3.7}\\]\nCon \\(\\pi = N_T/n\\) y \\(1 - \\pi = N_C/n\\).\nOperando con la Ecuaci√≥n¬†3.8 podemos despejar la diferencia entre los outcomes observables (SDO) y ver c√≥mo esta se realciona con el resto de las magnitudes definidas7.7¬†Pueden ver el despeje num√©rico en detalle en el cap√≠tulo 4 de (Cunningham 2021).\nCunningham, Scott. 2021. Causal inference: The mixtape. Yale university press.\n\n\\[\n\\begin{array}\n_E[Y_i^1|D_i=1] - E[Y_i^0|D_i=0] &=& ATE \\\\\n&+& ( E[Y_i^0|D_i=1] - E[Y_i^0|D_i=0] ) \\\\\n&+& (1-\\pi) (ATT - ATU)\n\\end{array}\n\\tag{3.8}\\]\nQue podemos reescribir como:\n\\[\n\\begin{array}\n_\\underbrace{\\frac{1}{N_T} \\sum_{i=1}^{N_T} (y_i|d_i=1) - \\frac{1}{N_C} \\sum_{i=1}^{N_C} (y_i|d_i=0)}_\\text{Diferencia de los outcomes} &=& \\underbrace{ATE}_\\text{Efecto promedio del tratamiento} \\\\\n&+& \\underbrace{( E[Y_i^0|D_i=1] - E[Y_i^0|D_i=0] )}_\\text{Sesgo de selecci√≥n} \\\\\n&+& \\underbrace{(1-\\pi) (ATT - ATU)}_\\text{Sesgo de efecto heterog√©neo}\n\\end{array}\n\\tag{3.9}\\]\nLo que puede verse en Ecuaci√≥n¬†3.9 es que si pudi√©ramos asegurar de alguna forma que los sesgos de selecci√≥n y de efecto heterog√©neo fueran cero, el SDO ser√≠a un buen estimador del ATE que es, al fin y al cabo, el efecto causal promedio que nos interesa en nuestro experimento."
  },
  {
    "objectID": "potential_outcomes.html#independencia",
    "href": "potential_outcomes.html#independencia",
    "title": "3¬† Potential outcomes",
    "section": "3.5 Independencia",
    "text": "3.5 Independencia\nLa definici√≥n de independencia en el contexto de los potential outcomes es la siguiente:\n\\[\n(Y^0, Y^1) \\perp D\n\\] Momento cerebrito, pensemos una poco qu√© quiere decir. Esto significa que la asignaci√≥n de los participantes al grupo control o tratamiento (\\(D\\)) no depende de los outcomes potenciales de ese individuo.\nVamos a pensarlo con un ejemplo concreto. Imaginen que tenemos una grupo de participantes para poner a prueba una cirug√≠a experimental como altenativa a un tratamiento m√©dico establecido no quir√∫rgico. Si la asingaci√≥n de individuos al grupo tratamiento la hace un m√©dico en base a lo que cree que va a ser conveniente para √©l, por ejemplo, no asignando a pacientes de edad avanzada al grupo tratamiento por el riesgo asociado a una cirug√≠a, o asignando a pacientes cuyo pron√≥stico con el m√©todo tradicional vea poco favorable al grupo control. En este caso, la asignaci√≥n a un grupo s√≠ depende de los posibles resultados, por lo tanto, no hay independencia. Si en lugar de eso tir√°ramos una moneda antes de recibir a cada paciente, podr√≠amos de esa forma asegurar la independencia.\nLa independencia implica que se cumpla:\n\\[\n\\begin{array}\n_E[Y^1|D=1] - E[Y^1|D=0] &=& 0 \\\\\nE[Y^0|D=1] - E[Y^0|D=0] &=& 0\n\\end{array}\n\\tag{3.10}\\]\nEs decir, que la esperanza de los outcomes para los participantes que fueron asignados al grupo tratamiento como al grupo control ser√≠an iguales si pudieramos medirlos a ambos en ‚Äúmundo tratamiento‚Äù8. Y lo mismo pasar√≠a si pudi√©ramos medirlos a ambos grupos en el ‚Äúmundo control‚Äù[^indep]. Ojo que esto no implica que la esperanza del outcome para tratamiento en los tratados sea igual a la esperanza para no tratamiento en los controles (\\(E[Y^1|D=1] - E[Y^0|D=0] = 0\\)) ni igual a la esperanza no tratamiento de los tratados (\\(E[Y^1|D=1] - E[Y^0|D=1] = 0\\)).8¬†Tengamos en cuenta que \\(E[Y^1|D=0]\\) es un contraf√°ctico, ya que es el outcome habiendo sido expuesto al tratamiento pero de los individuos en el grupo control (de ah√≠ la condicionalidad con \\(D=0\\))\n¬øQu√© implicancias tiene las igualdades presentadas en Ecuaci√≥n¬†3.10 en los sesgos que vimos en la ecuaci√≥n Ecuaci√≥n¬†3.8? Empecemos por el sesgo de selecci√≥n (\\(E[Y^0|D=1] + E[Y^0|D=0]\\)). Vemos que, deacuerdo a la primera l√≠nea de Ecuaci√≥n¬†3.10 ya nos dice que, de ser independiente la asignaci√≥n del grupo experimental, este sesgo ser√≠a cero. Pensemos un poco. Lo que nos est√° diciendo la condici√≥n de independencia es que si ambos grupos fueran no tratados, ambos tendr√≠an el mismo outcome lo que pareciera indicarnos que es razonable considerar nulo al sesgo de selecci√≥n.\nLa relaci√≥n del sesgo de efecto heterog√©neo (\\((1-\\pi) (ATT - ATU)\\)) con la independencia es un poquito m√°s dif√≠cil de demostrar. Olvid√©monos del \\((1-\\pi)\\) de momento. Reescribamos los efectos ATT y ATU:\n\\[\n\\begin{array}\n_ATT &=& E[Y^1|D=1] - E[Y^0|D=1] \\\\\nATU &=& E[Y^1|D=0] - E[Y^0|D=0]\n\\end{array}\n\\]\nY ahora restemos ambos t√©rminos:\n\\[\n\\begin{array}\n_ATT - ATU &=& E[Y^1|D=1] - E[Y^0|D=1] - ( E[Y^1|D=0] - E[Y^0|D=0] )\\\\\n&=& \\bigl\\{ E[Y^1|D=1] - E[Y^1|D=0] \\bigl\\} + \\bigl\\{ E[Y^0|D=0] - E[Y^0|D=1] \\bigl\\}\n\\end{array}\n\\tag{3.11}\\]\nReescrito de esta forma podemos ver los dos primero t√©rminos de Ecuaci√≥n¬†3.11 se hacen cero por la rpimero l√≠nea de Ecuaci√≥n¬†3.10, y los √∫ltimos dos se hacen cero por la segunda.\nFinalmente, demostramos que si hay independencia en la asignaci√≥n de los grupos, la diferencia de las medias entre el grupo tratado y el control es un buen estimador del ATE."
  },
  {
    "objectID": "intro_stat.html#probabilidad-condicional",
    "href": "intro_stat.html#probabilidad-condicional",
    "title": "2¬† Repaso de probabilidad y estad√≠stica",
    "section": "2.3 Probabilidad condicional",
    "text": "2.3 Probabilidad condicional\nLa probabilidad condicional es la probabilidad de que ocurra un evento A dado que ocurri√≥ un evento B y se escribe como \\(P(A|B)\\). Por ahora qued√©monos con esta definici√≥n simple que ser√° de vital importancia para lo que sigue."
  }
]