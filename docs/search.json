[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Diseños experimentales y cuasiexperimentales",
    "section": "",
    "text": "Este libro surge con la intención de acompañar a los estudiantes de la materia Diseño experimentales y cuasiexperimentales correspondiente a la carrera de Licenciatura en Ciencias del Comportamiento de la Universidad de San Andrés (Victoria, Argentina). Sin embargo, este libro no contiene todo los materiales de la materia, sino más bien es una guía que nos permite comprender conceptos básicos e identificar lo importante en cada tema. De forma complementaria, en cada capítulo se brindará una lista de bibliografía recomendada.\n\n\nEl objetivo de este libro es acercar a los lectores y lectoras los conceptos básicos del diseño experimental y cuasiexperimental para que de esta manera sean capaces de diseñar sus propios experimentos para expresar y verificar efectivamente hipótesis científicas. Asimismo se busca generar intuiciones que les permitan evaluar diseños experimentales con los que se puedan encontrar tanto en la literatura científica como en cualquier dato obtenido experimentalmente que se les presente al momento de tomar una decisión.\nSe espera que los lectores y lectoras sean capaces de comunicar los diseños experimentales, sus resultados y las implicancias de los mismos de manera clara, concisa y “apta para todo público”. Asímismo, un concepto transversal es que no siempre es posible implementar un diseño experimental “ideal” (si este existiera) y que tomar decisiones informadas sobre los mismos (modelos estadísticos, métricas, diseños, etc.) es una parte importante de nuestra labor como generadores de evidencia o tomadores de decisión basada en evidencia.\n\n\n\n\n\n\nSIGA SIGA…\n\n\n\nSi bien los ejemplos que se presentan en el libro están orientados a las ciencias del comportamiento, su contenido puede ser adaptado a cualquier ciencia experimental, desde la biología hasta la economía.\n\n\n\n\n\nEl libro se organiza en cuatro grandes secciones: Probabilidad y estadística, diseño experimental, diseño cuasi experimental y ejemplos de aplicación.\n\n\nEsta sección cuenta con dos capítulos:\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste libro no es un libro de inferencia estadística y no vamos a hablar de tests estadísticos sino en el contexto de una hipótesis científica y de un diseño experimental en particular. Es decir, este libro NO es un manual de estadística. Existen cientos de libros que desarrollan en detalle esos contenidos y para nada este libro pretende cubrir esos contenidos.\nEste libro tampoco es un manual de programación ni de análisis de datos en R. Si bien en la primera sección del libro haremos una presentación de algunas de las funcionalidades de la colección de paquetes para análisis de datos que es el tidyverse, no vamos a repasar ninguna de las bases de R ni de Rstudio. Existen infinidad de recursos invreíbles para aprender esto y sentimos que no hay necesida de, a eso, sumarle uno mediocre.\n\n\n\nPara sacarle el jugo a los contenidos de este libro hace falta tener conocimientos básicos de probabilidad y estadística así como estar familiarizado con la programación en R. Ninguno de estos son obstáculos hoy en día ya que hay cientos de fuentes (libros, cursos, etc.) a las que el lector puede consultar previo o durante la lectura de este libro.\n\n\n\n\n\n\nDON’T PANIC\n\n\n\nEl libro comienza con un breve repaso de conceptos básicos de probabilidad y estadística y presenta los comandos básicos para correr códigos en R.\n\n\n\n\n\n\n\n\nAnte cualquier duda pueden contactarme vía e-mail a ispiousas@udesa.edu.ar.\n\n\n\nEste sitio web es y siempre será gratuito, licencia bajo [CC BY-NC-ND 3.0 License]{https://creativecommons.org/licenses/by-nc-nd/3.0/us/}."
  },
  {
    "objectID": "intro_R.html",
    "href": "intro_R.html",
    "title": "1  R y el tidyverse",
    "section": "",
    "text": "En la gran mayoría de las ejemplos y ejercicios de este libro vamos a usar una computadora (te quiero mucho Skynet ♥️). Con ella nos vamos a comunicar utilizando un lenguaje de programación muy popular en el campo de la estadística: R(R Core Team 2023). Por eso mi recomendación es que lo que primero tenés que hacer es instalar R y RStudio. RStudio es una interfaz muy popular utilizada para, mayormente, programar en R. En el recuadro siguiente van a encontrar información de cómo instalar ambas cosas."
  },
  {
    "objectID": "intro_R.html#el-tidyverse",
    "href": "intro_R.html#el-tidyverse",
    "title": "1  R y el tidyverse",
    "section": "1.1 El tidyverse",
    "text": "1.1 El tidyverse\n\n1.1.1 Tidy data\nLo primero que tenemos que pensar cuando trabajamos con el tidyverse es que nuestros datos estén en formato tidy. ¿Qué significa esto? Cuando un dataset está en formato tidy, cada columna corresponde a una variable y cada fila a una única observación2. Veamos un ejemplo. Tenemos tres sujetos a los cuales les medimos el tiempo de respuesta en una tarea. Cada sujeto realiza dos repeticiones de esta medición, el trial 1 y el trial 2. En la tabla Table 1.1 podemos ver las dos formas de organizar esta información.2 El caso contrario sería en el que una fila contiene varios mediciones para distintos niveles de una variable. Este formato se conoce como wide.\n\n\nTable 1.1: Ejemplo de tablas tidy y wide.\n\n\n\n\n(a) Tidy \n \n  \n    sujeto \n    trial \n    tiempo_respuesta \n  \n \n\n  \n    Jerry \n    1 \n    0.0807501 \n  \n  \n    Jerry \n    2 \n    0.8343330 \n  \n  \n    Elaine \n    1 \n    0.6007609 \n  \n  \n    Elaine \n    2 \n    0.1572084 \n  \n  \n    George \n    1 \n    0.0073994 \n  \n  \n    George \n    2 \n    0.4663935 \n  \n\n\n\n\n\n\n(b) Wide \n \n  \n    sujeto \n    trial_1 \n    trial_2 \n  \n \n\n  \n    Jerry \n    0.4977774 \n    0.7725215 \n  \n  \n    Elaine \n    0.2897672 \n    0.8746007 \n  \n  \n    George \n    0.7328820 \n    0.1749406 \n  \n\n\n\n\n\n\nA lo largo de este capítulo iremos viendo los beneficios de almacenar los datos en formato tidy. Por supuesto que estas ventajas tienen su precio, principalemente que las bases de datos crecen mucho en tamaño si tenemos muchas medidas repetidas con distintos valores de las variables.\n\n\n1.1.2 Introducción al Tidyverse\nComo contamos más arriba, el tidyverse es una colección cerca de 25 paquetes, todos relacionados con la carga, manejo, modificación y visualización de datos. La idea de este libro no es profundizar en todas sus capacidades pero consideramos importante presentar algunas de las funciones que más vamos a utilizar a lo largo del libro. Estas son funciones para leer datos del paquete {readr}, los verbos de {dplyr} para manipularlos, las funciones de {tidyR} para acomodarlos y el poderosísimo {ggplot2} para visualizarlos.\n\n1.1.2.1 Cargando datos con readr\nUna de las cosas que vamos a hacer más a menudo en este libro es cargar algún dataset. Para esto vamos a usar varias de las funcionalidades del paquete {readr}.\nEl caso más simple al que nos vamos a enfrentar es la carga de una base de datos organizada en columnas y separadas por comas en un archivo de extensión .csv. En este caso lo que tenemos que hacer es bastante simple, usar la función read_csv() como a continuación:\n\ndata <- read_csv(\"../data/summer.csv\")\n#> Rows: 31165 Columns: 9\n#> ── Column specification ─────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (8): City, Sport, Discipline, Athlete, Country, Gender, Event, Medal\n#> dbl (1): Year\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nsummary(data)\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\nLos datos adentro de summer.csv son los ganadores de medallas en los juegos olímpicos de invierno. El formato en el que read_csv() almacena los datos se llama tibble y es el formato por excelencia del tidyverse. De momento lo único que nos importa es que es un formato que almacena los casos en filas y las variables en columnas (cada variable tiene un formato). Para más información sobre las cualidades de este formato, les recomiendo revisar la documentación.\n\n\n1.1.2.2 El operador pipe (|>) del paquete {magrittr}\nEl operador pipe nos permite concatenar funciones que utilizan como entrada los mismos datos. El principio de operación es el siguiente, supongan que nosotros queremos cargar un dataset y aplicarle la función summary. Esto lo podemos hacer simplemente cargando el dataset en una lìnea de código y ejecutanco la función summary() en la siguiente.\n\ndata <- read_csv(\"../data/summer.csv\")\n#> Rows: 31165 Columns: 9\n#> ── Column specification ─────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (8): City, Sport, Discipline, Athlete, Country, Gender, Event, Medal\n#> dbl (1): Year\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nsummary(data)\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\nPero, también podemos aprovechar el operador pipe y hacer todo en una única línea de código.\n\nread_csv(\"../data/summer.csv\") |> summary()\n#> Rows: 31165 Columns: 9\n#> ── Column specification ─────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (8): City, Sport, Discipline, Athlete, Country, Gender, Event, Medal\n#> dbl (1): Year\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\nAl dejar vacío el paréntesis de la función summary(), la misma va a tomar como variable de entrada a la que está antes del operador pipe, es decir, a la que antes llamamos data. En el caso que la función summary() tuviera más de una variable de entrada, lo que viene antes del pipe tomaría el lugar de la primera de ellas.\nSi bien esta funcionalidad parece algo que complica las cosas y que no trae demasiados beneficios con un ejemplo tan simple, más adelante veremos que puede ser de gran utilidad, ayudando a disminuir la cantidad de línes de código y de variables intermedias.\n\n\n1.1.2.3 Dplyr y sus verbos\nUna de las cosas más útiles del Tidyverse para el tipo de procesamiento de datos que vamos a llevar a cabo en este libro son los verbos de dplyr. Estas funciones no permiten agregar columnas, resumir la información, filtrar filas, seleccionar columnas, etc. Y todas estas acciones las podemos hacer en la base de datos completa o en una parte de ella agrupada de acuerdo a algún criterio. Vayamos de a poco.\nhttps://dplyr.tidyverse.org/\n\n\n1.1.2.4 TidyR, el paquete para ordenar tus datos\n\n\n1.1.2.5 ggplot2 o cómo hacer figuras que sean la envidia de tu compañero de escritorio"
  },
  {
    "objectID": "intro_R.html#cierre",
    "href": "intro_R.html#cierre",
    "title": "1  R y el tidyverse",
    "section": "1.2 Cierre",
    "text": "1.2 Cierre\nComo vimos brevemente en este capítulo, los paquetes del tidyverse son una herramineta importantísima para el análisis de datos utilizando R. Para más detalles sobre estas funcionalidades les recomendamos la guía de Hadley Wickham(Wickham et al. 2019) o, si ya se quieren sumergir de lleno en el mundo del análisis de datos con R, este fantástico libro [Wickham, Çetinkaya-Rundel, and Grolemund (2023)]3. Es decir, sin tener que cargar ningún paquete de funciones adicional..3 Disponible gratis online en este link https://r4ds.had.co.nz/.\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686.\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "intro_stat.html#variables-aleatorias",
    "href": "intro_stat.html#variables-aleatorias",
    "title": "2  Repaso de probabilidad y estadística",
    "section": "2.1 Variables aleatorias",
    "text": "2.1 Variables aleatorias\nWasserman(Wasserman 2004) nos dice que una variable aleatoria es un mapeo entre el espacio de eventos y los números reales (\\(X:\\Omega \\rightarrow \\mathbb{R}\\)). Momento cerebrito ¿Esto que quiere decir? En términos prácticos, lo que implica es definición es que una variable aleatoria nos da un número para cada evento del posible espacio de eventos.\nVamos con un ejemplo. Supongan que tiramos una moneda justa dos veces y tenemos la variabla aleatoria \\(X\\) que cuenta la cantidad de caras (H)1. Los posibles eventos \\(\\omega\\) del espacio de eventos \\(\\Omega\\) son \\(\\Omega = \\{ TT, TH, HT, HH \\}\\). En este caso, la variable aleatoria \\(X\\) va a tomar los valores \\(X = \\{ 0, 1, 1, 2\\}\\) para cada \\(\\omega\\). Esto, en resumidas cuentas, es lo que hace una variable aleatoria.1 Del inglés Head y ceca sera T del inglés Tail.\nEl ejemplo anterior se trata de una variable aleatoria discreta, es decir, que sólo puede tomar algunos valores posibles, pero también existen variables aleatorias continuas como por ejemplo la altura de una nueva persona que nace."
  },
  {
    "objectID": "intro_stat.html#probabilidad",
    "href": "intro_stat.html#probabilidad",
    "title": "2  Repaso de probabilidad y estadística",
    "section": "2.2 Probabilidad",
    "text": "2.2 Probabilidad\nPara una variable como la definida en el ejemplo anterior podemos definir fácilmente su probabilidad de ocurrencia. Debido que la moneda es justa, todos los eventos de \\(\\Omega\\) son equiprobables, y es por eso que podemos definir:\n\n\n\n\\(\\omega\\)\n\\(X(\\omega)\\)\n\\(P({\\omega})\\)\n\n\n\n\nTT\n0\n1/4\n\n\nTH\n1\n1/4\n\n\nHT\n1\n1/4\n\n\nHH\n2\n1/4\n\n\n\nSin embargo, el concepto de probabilidad es algo complejo, pero, como esto no es un curso de probabilidad, vamos a confiar en que ustedes ya lo traen claro. Si tienen coraje puede ir a leer el capítulo 1 de (Wasserman 2004) y si quiere algo más terrenal pueden ir a ver el repaso de probabilidad de (Cunningham 2021) (disponible online).\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale university press.\nCuando las variables aleatorias son continuas la cosa se complica un poco más ya que \\(P(X=c)\\), es decir, la probabilida de que una variable tome un valor dado, es cero. Esto lo vamos a repensar un poco en la siguiente sección, cuando definamos lo que nos importa para este libro: Las funciónes de densidad y de distribución."
  },
  {
    "objectID": "intro_stat.html#eventos-y-probabilidad-condicional",
    "href": "intro_stat.html#eventos-y-probabilidad-condicional",
    "title": "2  Repaso de probabilidad y estadística",
    "section": "2.3 Eventos y probabilidad condicional",
    "text": "2.3 Eventos y probabilidad condicional\nAcá agregar lo de árboles y diagramas.\nPensemos en un ejemplo"
  },
  {
    "objectID": "intro_stat.html#probabilidad-total",
    "href": "intro_stat.html#probabilidad-total",
    "title": "2  Repaso de probabilidad y estadística",
    "section": "2.4 Probabilidad total",
    "text": "2.4 Probabilidad total\nAhora imaginemos que pasa si queremos calcular la probabilidad de B (\\(P(B)\\)). Bueno, para esto tendríamos que considerar la probabilidad de que ocurra B dado que ocurrió S y junto con la probabilidad de B dado que NO ocurrió S. A su vez, cada una de estas probabilidades deberíamos pesarlas por la probabilidad de que ocurra o no A. Esto sería:\n\\[\nP(B) = P(B|A) \\times P(A) +  P(B|\\neg A) \\times P(\\neg A)\n\\]\nVolvamos al ejemplo de las cartas. En este caso, si calculamos la probabilidad de B obenemos:\n\\[\nP(B) = XXX\n\\]\nDe forma general definimos a la probabilidad total como:\n\\[\nblabla\n\\]"
  },
  {
    "objectID": "intro_stat.html#teorema-de-bayes",
    "href": "intro_stat.html#teorema-de-bayes",
    "title": "2  Repaso de probabilidad y estadística",
    "section": "2.5 Teorema de Bayes",
    "text": "2.5 Teorema de Bayes\nAhora que ya llegamos a la fórmula de Bayes a partir de las definiciones de probabilidad total podemos tomar prestado un ejemplo de (Herzog, Francis, and Clarke 2019): Tenemos un test para identificar si somos portadores de un virus (llamémoslo IKV). Este test tiene una sensibilidad de 99.99% y una especificidad de 99.99%. Es decir, la probabilidad de que el test de positivo, dado que tenemos el virus (\\(P(T^+|IKV)\\)) es de 0.9999, y lo mismo ocurre para la probabilidad de que el test de negativo en caso de que NO tengamos el virus (\\(P(T^-|\\neg IKV)\\)). Sabemos también que la incidencia del virus IKV es de 1 en 10000.\n\nHerzog, Michael H, Gregory Francis, and Aaron Clarke. 2019. Understanding Statistics and Experimental Design: How to Not Lie with Statistics. Springer Nature.\nSupongamos que somos elegidos aleatoriamente para realizarnos el test y este da positivo ¿Qué probabilidad de ser portadores del virus tenemos (\\(P(IKV|T^+)\\))? La primera respuesta que se nos viene es 0.9999 ¿Verdad? Pero, si estuvimos prestando atención, ya a esta altura debemos saber que para invertir la condicionalidad de una probabilidad tenemos que acudir al bueno de Bayes. O sea:\n\\[\nP(IKV|T^+) = \\frac{P(T^+|IKV) \\times P(IKV)}{P(T+)}\n\\]\ndonde \\(P(T^+|IKV) = 0.9999\\) y \\(P(IKV) = 1/10000 = 0.0001\\). Además, echando mano a la definición de probabilidad total podemos calcular \\(P(T+)\\) como:\n\\[\nP(T+) = P(T^+|IKV) \\times P(IKV) + P(T^+|\\neg IKV) \\times P(\\neg IKV)\n\\]\ndonde \\(P(T^+|\\neg IKV) = 1-0.9999\\) y \\(P(\\neg IKV) = 1-0.0001\\). Reemplazando todos los valores tenemos que:\n\\[\n\\begin{array}\n_P(IKV|T^+) &=& \\frac{P(T^+|IKV) \\times P(IKV)}{P(T+)}\\\\\n&=& \\frac{P(T^+|IKV) \\times P(IKV)}{P(T^+|IKV) \\times P(IKV) + P(T^+|\\neg IKV) \\times P(\\neg IKV)} \\\\\n&=& \\frac{0.9999 \\times 0.0001}{0.9999 \\times 0.0001 + (1-0.9999) \\times (1-0.0001)} \\\\\n&=& \\frac{0.9999 \\times 0.0001}{0.9999 \\times 0.0001 + 0.0001 \\times 0.9999} \\\\\n&=& 0.5\n\\end{array}\n\\]\n¿Qué? ¿Esto significa que si el test me da positivo solo tengo un 0.5 de probabilidad de tener el virus? ¿Esto quiere decir que los tests no sirven para nada? Momento, analicemos un poco al resultado al que llegamos. Lo que nos dice esta cuenta es que, una vez que el test nos da positivo, a pesar de lo sensible del test y por lo “raro” de la portación del virus, nuestra probabilidad de ser portadores es de 0.5. Pero, ¿Y nuestra probabilidad de ser portadores si el test nos da negativos? Hagamos la cuenta:\n\\[\n\\begin{array}\n_P(IKV|T^-) &=& \\frac{P(T^-|IKV) \\times P(IKV)}{P(T-)}\\\\\n&=& \\frac{P(T^-|IKV) \\times P(IKV)}{P(T^-|IKV) \\times P(IKV) + P(T^-|\\neg IKV) \\times P(\\neg IKV)} \\\\\n&=& \\frac{(1-0.9999) \\times 0.0001}{(1 - 0.9999) \\times 0.0001 + (0.9999) \\times (1-0.0001)} \\\\\n&=& 1E-8\n\\end{array}\n\\]\nOK, ahora la cosa tiene más sentido. O sea, el test es bastante bueno para decirnos cuando no somos portadores y dando negativo, el problema es cuando da positivo. En este caso tenemos que preocuparnos, pero, como vimos anteriormente, la probabilidad de ser portadores es de apenas 0.5.\nHay una solución más simple para esto y es la que deben estar pensando ustedes: ¿Y si me hacen un segundo test? ¡BINGO! Calculemos rápidamente la probabilidad de estar infectados si nos testean por segunda vez:\n\\[\nP(IKV|T^{2+}) = \\frac{0.9999^2 \\times 0.0001}{0.9999^2 \\times 0.0001 + 0.0001^2 \\times 0.9999} = 0.9999\n\\] Ahora sí, si somos testeados por segunda vez, la probabilidad de ser portadores dado que tenemos dos resultados positivos trepa a 0.9999. Nos podemos quedar tranquilos.\nPara cerrar, me gustaría que pensemos un poco en una palabra MUY importante que se dijo en el enunciado del problema: Aleatoriamente. En muchos de los casos en los que nos testeamos para ver si somos portadores de un virus, lo hacemos porque tenemos algún tipo de presunción de que podemos serlo (por ejemplo, tenemos síntomas). ¿Cuál creen que sería la probabilidad que se modifica en la fórmula? Exacto, \\(P(IKV)\\), ya que sería más bien \\(P(IKV|síntomas)\\)."
  },
  {
    "objectID": "intro_stat.html#esperanza",
    "href": "intro_stat.html#esperanza",
    "title": "2  Repaso de probabilidad y estadística",
    "section": "2.6 Esperanza",
    "text": "2.6 Esperanza\nLa esperanza de una variable aleatoria \\(X\\), a veces también llamada media poblacional, es simplemente la suma pesada de todos sus valores posibles. No debemos confundir la esperanza con el promedio muestral, aunque, como veremos en breve, para algunos casos el primero es un estimador insesgado del segundo.\nLa esperanza de una variable aleatoria discreta se define como:\n\\[\nE(X) = \\sum_{1}^\\infty x_i p(x_i)\n\\] En este caso es muy claro la naturaleza de promedio pesado, ya que a cada valor posible de \\(X\\) lo estamos pesando por su probabilidad. Sin embargo, para una variable aleatoria continua, en la que no tenemos definida una probabilidad puntual \\(p(x_i)\\) sino una función de densidad \\(f(x)\\), la definición es la siguiente:\n\\[\nE(X) = \\int_{-\\infty}^\\infty x f(x) dx\n\\]\nComo resulta esperable, la suma se transforma en una integral y la probabilidad puntual se reemplaza por \\(f(x)\\).\nAlgunas propiedades importantes de la esperanza son:\n\\[\n\\begin{array}\n_E(aX+b) & = & aE(X) + b\\\\\nE(\\sum_{i=1}^n X_i) & = & \\sum_{i=1}^nE(X_i)\n\\end{array}\n\\] Por último y a modo de aviso, advertencia y amenaza, recordemos que \\(E(X)^2 \\neq E(X^2)\\)."
  },
  {
    "objectID": "intro_stat.html#varianza-y-covarianza",
    "href": "intro_stat.html#varianza-y-covarianza",
    "title": "2  Repaso de probabilidad y estadística",
    "section": "2.7 Varianza y covarianza",
    "text": "2.7 Varianza y covarianza\n\\[\nV(X) = \\sigma^2 = E \\left[ (X - E(X))^2 \\right]\n\\] Y se puede demostrar que:\n\\[\nV(X) = E(X^2) - E^2(X)\n\\]"
  },
  {
    "objectID": "intro_stat.html#correlación",
    "href": "intro_stat.html#correlación",
    "title": "2  Repaso de probabilidad y estadística",
    "section": "2.8 Correlación",
    "text": "2.8 Correlación"
  },
  {
    "objectID": "intro_stat.html#población-y-muestra",
    "href": "intro_stat.html#población-y-muestra",
    "title": "2  Repaso de probabilidad y estadística",
    "section": "2.9 Población y muestra",
    "text": "2.9 Población y muestra\nAcá usar una versión de la figurita de All of statistics que pone la generación de los datos en el dominio de la probabilidad y la estimación de estos parámetros en el dominio de la estadística. Me parece una forma ideal de empezar a hablar de qué queremos hacer con la estadística.\nVamos con un ejemplo que nos puede ayudar a entender de qué hablamos cuando hablamos de estimación. Supongamos que conocemos distribución de la altura de la población de varones en Argentina. No estamos hablando de calcular el promedio de la altura de los varones sino de que conocemos la función de densidad de la cual la altura de cada varón es una muestra. Si no queda del todo claro respiren hondo y esperen un poco que ya se va a ir aclarando. Entonces, la altura de los varones de Argentina tiene una distribución normal con media en cm. de \\(\\mu_{varones} = 175\\) y una desviación estándar \\(\\sigma_{varones} = 7\\), o, como ya aprendimos a decir: \\(H_{varones} \\sim \\mathcal{N}(\\mu_{varones},\\sigma^2_{varones}) = \\mathcal{N}(175, 49)\\)2. A continuación podemos ver la función de densidad:2 h del inglés height.\n\n\n\n\n\n\n\n\n\nAhora bien, en la figura podemos ver la función \\(f(x)\\) junto con una línea vertical que nos indica la media y dos líneas que nos indican los percentiles \\(2.5\\) y \\(97.5\\), es decir, que contienen el 95% de la masa de probabilidad. Todo esto es muy lindo pero estamos jugando a ser dios (o el Doctor Manhattan, o en lo que ustedes crean). Es imposible conocer los parámetros de esta distribución pero lo que sí podemos hacer en la práctica es estimarlos. Estimar los parámetros de un modelo es el pan y manteca de la inferencia estadística y el data mining. Como podemos ver en esta hermosa figura de Wasserman(Wasserman 2004), la teoría de probabilidad nos ayuda a definir modelos para la generación de datos y la inferencia estadística nos ayuda a estimar estos parámetros.\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media.\nHay diversas formas de encontrar estimadores para los parámetros de un modelo (por ejemplo, método de los momentos, máxima verosimilitud, etc.) pero entenemos que eso excede los contenidos de este curso. Sin embargo, para estimar todos conocemos los estimadores de los parámetros poblacionales \\(\\mu\\) y \\(\\sigma^2\\). Claro, el promedio \\(\\bar{x}\\) y el desvío muestral \\(\\hat{S}^2\\):\n\\[\n\\begin{array}\n\\\\\\bar{x} & = & n^{-1} \\sum_{i=1}^n x_i\\\\\n\\hat{S}^2 & = & (n-1)^{-1} \\sum_{i=1}^n (x_i\n\\end{array}\n\\]\nSimulemos tres experimento tomando 10, 50 y 100 mediciones (\\(n\\)) y veamos los histogramas de estas muestras y sus estimaciones de \\(\\mu\\) y \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\nComo es de esperarse, podemos ver que al aumentar \\(n\\), la estimación de los parámetros poblacionales es mejor. Pero tenemos que tener esta idea en mente, cada vez que tomamos una muestra podemos estimar un parámetro de la población y hasta hacer inferencias estadísticas sobre el mismo, pero NUNCA lo vamos a conocer."
  },
  {
    "objectID": "potential_outcomes.html",
    "href": "potential_outcomes.html",
    "title": "3  Potential outcomes",
    "section": "",
    "text": "En construcción 🚧"
  },
  {
    "objectID": "potential_outcomes.html#sdf",
    "href": "potential_outcomes.html#sdf",
    "title": "3  Potential outcomes",
    "section": "3.1 Sdf",
    "text": "3.1 Sdf"
  },
  {
    "objectID": "dags.html",
    "href": "dags.html",
    "title": "4  Grafos acíclicos dirigidos (DAGS)",
    "section": "",
    "text": "Los grafos acíclicos dirigidos 1son una herramienta para representar visualmente las relaciones causales presentes en nuestro diseño experimental, ni más ni menos..1 A partir de ahora DAGS, del inglés Directed acyclic graphs."
  },
  {
    "objectID": "dags.html#definiciones-características-y-ejemplos",
    "href": "dags.html#definiciones-características-y-ejemplos",
    "title": "4  Grafos acíclicos dirigidos (DAGS)",
    "section": "4.1 Definiciones, características y ejemplos",
    "text": "4.1 Definiciones, características y ejemplos\nUn DAG es una representación gráfica de una cadena de efectos causales. Los nodos (los circulitos o cuadraditos que vamos a ver más adelante) representan variables aleatorias y las flechas que los unen representan la relación causal que se mueve de una variable a la otra en la dirección intuitiva de la flecha. Por ejemplo, pensemos que queremos estudiar el efecto de tomar una aspirina en la intensidad de nuestro dolor de cabeza cuando nos duele la cabeza. Tenemos dos variables Aspirina (A) e Intensidad del dolor (I). Esta relación la podemos expresar en el siguiente DAG:\n\n\n\n\n\nDAG que representa la relación entre tomar una aspirina (A) y la intensidad del dolor (I).\n\n\n\n\n\n\n\n\n\n\nDAGS en R\n\n\n\nPara realizar los DAGS que aparecen en este capítulo utilizamos la librería {ggdag}. Para esto primero debemos hacer un esquema de nuestro DAG en Daggity y luego copiar parte de lo generado en la definición de nuestro DAG en R. En el desarrollo del capítulo utilizaremos algunas de las herramientas de {ggdag}, varias de las cuales pueden encontrar en este tutorial. Sin embargo, para una revisión pormenorizada de sus funciones recomiendo repasar la documentación del mismo.\n\n\n¿Así de simple? Sí y no ¿Qué pasa cuando las cosas se empiezan a complicar? Pensemos en el clásico ejemplo del mantra “correlación no implica causalidad” la relación entre venta de helados (Hel) y accidentes por mordidas de tiburón en Australia(Sh). Si nosotros planteáramos que el aumento de la venta de helados causa el aumento de los accidentes no tendría mucho sentido, ¿No?. Entonces, ¿Qué está pasando? ¿Qué pinta tiene el DAG? Bueno, como ya comentamos cuando hablamos de correlación en este caso lo que tenemos es una común a ambos fenómenos que, a modo de simplificar, la podríamos resumir como la temperatura (T). Entonces, cuando sube la temperatura sube la venta de helados, pero también los accidentes por mordidas de tiburón. Una posible relación causal entre ambas variables podría ser esta:\n\n\n\n\n\nDAG que representa la relación entre las ventas de helados (Hel), los accidentes por mordida de tiburón (Sh) y la temnperatura (T) en las playas de Australia.\n\n\n\n\nTodo muy lindo, pero ¿Para qué?"
  },
  {
    "objectID": "dags.html#el-criterio-de-las-puertas-de-atrás",
    "href": "dags.html#el-criterio-de-las-puertas-de-atrás",
    "title": "4  Grafos acíclicos dirigidos (DAGS)",
    "section": "4.2 El criterio de las puertas de atrás",
    "text": "4.2 El criterio de las puertas de atrás\nUna de las principales ventajas de plantear un DAg para estudiar una relación causal es que nos permite ajustar un modelo a partir del cual, lo que estamos estimando en uno de sus parámetros es un estimador de la relación causal que queremos estudiar. Empecemos planteando los caminos posibles para llegar desde Hel a Sh. En este caso serían dos:\n\\[\n\\begin{array}\n_Hel    \\longrightarrow Sh \\\\\nHel \\longleftarrow T \\longrightarrow Sh\n\\end{array}\n\\]\nEl primero es lo que se llama un camino directo y es lo relación causal que queremos estudiar. Por otro lado, el segundo (\\(Hel \\leftarrow T \\rightarrow Sh\\)) es lo que se llama una puerta de atrás y lo identíficamos porque, al menos en alguna de sus relaciones causales, la flechita va para la izquierda. Identificar estos caminos puerta de atrás es una parte fundamental de controlar la variabilidad espúrea en nuestras relaciones causales. En particular en este ejemplo, tenemos un claro confusor y lo que queremos es controlar por T.\nSimulemos esta relación y veamos que pasa.\n\nset.seed(1414)\nhelados <- tibble(\n  Temperatura = rnorm(1000, 20, 5),\n  Helados     = 1 + 1*Temperatura + rnorm(1000),\n  Ataques     = 1 + 2*Temperatura + 0*Helados + rnorm(1000)\n)\n\nVeamos que Temperatura es una variable aleatoria con distribución normal con media \\(20\\) y desviación estándar \\(5\\). Tomamos \\(1000\\) muestras de la misma y después generamos las variables Helados y Ataques como una combinación lineal de Temperatura más un error aleatorio de media \\(0\\) y desviación estándar \\(1\\). En la definición de Ataques podemos ver explícitamente que la influencia de Helados en Ataques es \\(0\\). Sin embargo, miremos la relación que existe entre ambas variables:\n\n#> `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nDAG que representa la relación entre las ventas de helados (Hel), los accidentes por mordida de tiburón (Sh) y la temnperatura (T) en las playas de Australia.\n\n\n\n\nVemos que ambas variables están altamente correlacionadas, de hecho, su r de Pearson vale 0.977. Sin embargo, nosotros sabemos que esa correlación es espúrea, que no hay una relación causal entre ventas de helados y ataques de tiburón y que algo tenemos que hacer. Hemos escuchado muchas veces que lo que tenemos que hacer es “controlar” por la temperatura, lo que en el contexto de la regresión lineal no significa otra cosa que agergar Temperatura como una covariable. Ajustemos dos modelos de regresión, uno con la Temperatura como covariable y uno sin y comparemos las estimaciones de los efectos causales (\\(\\hat\\beta_H\\))2:2 Donde \\(\\epsilon_i\\) y \\(\\tau_i\\) son los términos de error i.i.d. distribuidos normalmente con \\(\\mu=0\\) y \\(\\sigma=1\\).\n\\[\n\\begin{array}\n_lm_1&:& Ataques_i = \\alpha + \\beta_{H} Helados_i + \\epsilon_i \\\\\nlm_2&:& Ataques_i = \\alpha + \\beta_{H} Helados_i +  \\beta_{T} Temperatura_i + \\tau_i\n\\end{array}\n\\] En la siguiente tabla podemos ver las estimaciones de \\(\\hat\\beta_H\\) para cada uno de los modelos:\n\n\nEstimaciones de los parámetros de los modelos lm1 y lm2 definidos anteriormente.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtaques\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSin controlar por Temp.\n\n\n\n\n\n\n\n\n\n\n\n\nControlando por Temp.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\n\n\n\n\n\n\n\n\n(2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHelados\n\n\n\n\n\n\n\n\n\n\n\n\n1.928***\n\n\n\n\n\n\n\n\n\n\n\n\n-0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\n(0.032)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTemperatura\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.003***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.033)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n\n\n\n\n\n\n\n\n\n\n0.602**\n\n\n\n\n\n\n\n\n\n\n\n\n1.015***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.291)\n\n\n\n\n\n\n\n\n\n\n\n\n(0.134)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n\n\n\n\n\n1,000\n\n\n\n\n\n\n\n\n\n\n\n\n1,000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR2\n\n\n\n\n\n\n\n\n\n\n\n\n0.954\n\n\n\n\n\n\n\n\n\n\n\n\n0.990\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjusted R2\n\n\n\n\n\n\n\n\n\n\n\n\n0.954\n\n\n\n\n\n\n\n\n\n\n\n\n0.990\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidual Std. Error\n\n\n\n\n\n\n\n\n\n\n\n\n2.246 (df = 998)\n\n\n\n\n\n\n\n\n\n\n\n\n1.032 (df = 997)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF Statistic\n\n\n\n\n\n\n\n\n\n\n\n\n20,826.210*** (df = 1; 998)\n\n\n\n\n\n\n\n\n\n\n\n\n51,243.220*** (df = 2; 997)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\n\n\n\n\n\n\n\n\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo podemos ver, si no controlamos por Temperatura , la estimación de \\(\\hat\\beta_H\\) tiene un valor cercano a \\(1\\), mientras que si controlamos por Temperatura tiene un valor cercano a \\(0\\) lo que sabemos es el valor “real” del parámetro \\(\\beta_H\\). Todo muy lindo pero, ¿Qué tiene que ver esto con los DAGS? Bueno, cuando planteamos un DAG existe algo que se llama el criterio de las puertas traseras que dice que para estimar el efecto causal que nos interesa debemos “cerrar” todas las puertas traseras que conectan la causa y elefecto. Y “cerrar” en este contexto es simplemente controlar por la variabilidad de alguna de las variables presentes en la puerta trasera. En este caso, controlando por Temperatura estamos cerrando la única puerta trasera, por lo tanto, estamos estimando el verdadero efecto causal que nos interesa."
  },
  {
    "objectID": "dags.html#colliders",
    "href": "dags.html#colliders",
    "title": "4  Grafos acíclicos dirigidos (DAGS)",
    "section": "4.3 Colliders",
    "text": "4.3 Colliders\nHasta ahora tuvimos que lidiar casi únicamente con confusores pero existe otro tipo de variables en el contexto de un camino causal que se denomina collider. Vemos un ejemplo y apliquemos el criterio de las puertas traseras. Pensemos el siguiente ejemplo. Supongamos que queremos estudiar el efecto del factor de riesgo edad (Age) en la infección de COVID-19 (Cov), pero lo hacemos a través de datos voluntarios recavados por una aplicación móvil (App). Un DAG muy simplicado que podríamos plantear es el siguiente:\n\n\n\n\n\nDAG que representa la relación entre la edad (Age), la infección por COVID-19 (Cov) y el uso de la aplicación móvil de autoreporte (App).\n\n\n\n\nYa uqe sabemos que la edad tienen un. efecto en el uso de aplicaciones móviles y, podemos suponer, que la gente que se infecta de COVID-19 tiende a reportar más sus datos en la aplicación. Ahora veamos los caminos:\n\\[\n\\begin{array}\n_Age    \\longrightarrow Cov \\\\\nAge \\longrightarrow App \\longleftarrow Cov\n\\end{array}\n\\] Repitamos el ejercicio de simulación que utilizamos en el ejemplo de los confusores, sólo que esta vez Covid es una variable dicotómica y, por lo tanto, debemos muestrarla de una distribución Bernoulli3:3 Para más detalles sobre como simular una variiable con distribución Bernoulli (binomial con \\(n=1\\)) podemos ver el siguiente link.\n\nset.seed(123)\ncovid <- tibble(\n  Age   = rnorm(1000, 40, 10),\n  Covid = rbinom(1000, 1, prob = 1/(1+exp(10-.25*Age))),\n  App   = 1 - 1*Age + 1*Covid +rnorm(1000)\n)\n\nPuede verse que la verdadera relación entre Covid y Age (en términos de parámetros de una regresión logística) es \\(0.25\\). Veamos como se ve la edad de los infectados y no infectados:\n\n\n\n\n\nInfefctados y no infectados de COVID-19 en función de la edad.\n\n\n\n\nSegún el criterio de las puertas traseras deberíamos controlar por App para así cerrar ese camino. Ajustemos estos dos regresiones logísticas y veamos sus estimaciones de los parámetros:\n\\[\n\\begin{array}\n_glm_1&:& logit(Covid_i) = \\alpha + \\beta_{Age} Age_i + \\epsilon_i \\\\\nglm_2&:& logit(Covid_i) = \\alpha + \\beta_{Age} Age_i + \\beta_{App} App_i + \\epsilon_i\n\\end{array}\n\\]\nEn la siguiente tabla podemos ver las estimaciones de \\(\\hat\\beta_{Age}\\) para cada uno de los modelos:\n\n\nEstimaciones de los parámetros de los modelos glm1 y glm2 definidos anteriormente.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSin controlar por App\n\n\n\n\n\n\n\n\n\n\n\n\nControlando por App\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\n\n\n\n\n\n\n\n\n(2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\n\n\n\n\n\n\n\n0.243***\n\n\n\n\n\n\n\n\n\n\n\n\n1.354***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.016)\n\n\n\n\n\n\n\n\n\n\n\n\n(0.109)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.100***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.103)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n\n\n\n\n\n\n\n\n\n\n-9.787***\n\n\n\n\n\n\n\n\n\n\n\n\n-11.839***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.631)\n\n\n\n\n\n\n\n\n\n\n\n\n(0.763)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n\n\n\n\n\n1,000\n\n\n\n\n\n\n\n\n\n\n\n\n1,000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog Likelihood\n\n\n\n\n\n\n\n\n\n\n\n\n-419.336\n\n\n\n\n\n\n\n\n\n\n\n\n-344.156\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n\n\n\n\n\n\n\n\n\n\n842.672\n\n\n\n\n\n\n\n\n\n\n\n\n694.312\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\n\n\n\n\n\n\n\n\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa estimación del efecto correcta sería la del modelos sin controlar pero ¿Por qué pasa esto? Cuando tenemos un collider podemos considerar ese camino como cerrado por defecto y al controlar por él, ese camino se abre. Entonces, en si bien teníamos dos posibles caminos causales, sólo uno estaba abierto y no necesitábamos controlar por App. Esto se debe a que, al App no causar ninguna de mis otras dos variables, ese camino causal está cerrado. Para reflexionar un poco en por qué ese camino se abre al controlar por un collider recomiendo las reflexiones del capítulo 8 de (Huntington-Klein 2021).\n\n\n\n\n\n\nEl criterio de las puertas traseras\n\n\n\nEn resumen, el criterio de las puertas traseras nos dice que para estimar la relación causal principal debemos cerrar todas las puertas traseras (caminos causales entre la causa y el efecto que queremos estudiar que tienen alguna flecha hacia atrás). Recordemos que para cerrar esos caminos debemos controlar por alguna de las variables que lo componen, agragándola como covariable a nuestro modelo estadístico. Por último, recordemos que cuando tenemos un collider el camino està cerrado y al controlar por él lo abrimos."
  },
  {
    "objectID": "dags.html#un-ejemplo",
    "href": "dags.html#un-ejemplo",
    "title": "4  Grafos acíclicos dirigidos (DAGS)",
    "section": "4.4 Un ejemplo",
    "text": "4.4 Un ejemplo\nAnalicemos el ejemplo que plantean los autores en (Tönnies, Kahl, and Kuss 2022). Lo que quieren estudiar es el efecto causal de la obesidad (Ob) end la mortalidad (Mort). Pero, identifican dos variables que tambièn podrìan estar afectando: 1- Si el individuo tiene diabetes (Diab) y 2- Si el individuo es fumador (Smok). La relación causal enntre las variables que plantean (medio de juguete) puede verse representada en el siguiente DAG4:4 Esto resulta especialmente útil cuando los DAGS se empiezan a complicar.\n\ndag_Ob <- dagitty::dagitty('dag {\nOb [exposure,pos=\"-1.5,0\"]\nMort [outcome,pos=\"1.5,0\"]\nSmok [pos=\"1.5,1.5\"]\nDiab [pos=\"0,1.5\"]\nOb -> Mort\nOb -> Diab\nDiab -> Mort\nSmok -> Mort\nSmok -> Diab\n}')\n\ntidy_dag <- tidy_dagitty(dag_Ob)\nggdag(tidy_dag) +\n  theme_dag()\n\n\n\n\nDAG que representa la relación causal planteada por los autores en (Tönnies, Kahl, and Kuss 2022).\nTönnies, Thaddäus, Sabine Kahl, and Oliver Kuss. 2022. “Collider Bias in Observational Studies: Consequences for Medical Research Part 30 of a Series on Evaluation of Scientific Publications.” Deutsches Ärzteblatt International 119 (7): 107.\n\n\n\n\n\nSi tebemos todos estos datos observados y queremos estimar el efecto causal de la obesidad en la mortalidad, lo primero que tenemos que hacer es plantear todos los caminos abiertos. Esto lo podemos hacer a ojo, pero también nos podemos ayudar con la función ggdag_paths del paquete {ggdags}[^dags-3]. A continuación vemos un ejemplo de uso de esta función:\n\ndag_Ob %>% ggdag_paths(from = \"Ob\", to = \"Mort\", shadow = TRUE) + \n  theme_dag() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nUsando ggdag_collider para identificar colliders en nuestro DAG.\n\n\n\n\nPodemos ver que los caminos abiertos son:\n\\[\n\\begin{array}\n_Ob  \\longrightarrow Mort\\\\\nOb  \\longrightarrow Diab \\longrightarrow Mort\n\\end{array}\n\\]\nPero ¿Por qué no está abierto el camino \\(Ob \\rightarrow Diab \\leftarrow Smok \\rightarrow Mort\\)? ¡Exacto! Está cerrado, porque, para ese camino, Diab es un collider ya que está causada tanto por Smok como por Ob. Resulta importante notar que una variable es un collider o no en el contexto de un camino de causalidad y no lo es siempre. De hecho, podemos ver que Diab actúa como un confusor en el camino \\(Ob \\rightarrow Diab \\rightarrow Mort\\). La función ggdag_collider también nos puede ayudar a identificar un collider.\n\ndag_Ob %>% ggdag_collider() +\n     theme_dag() +\n     scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nUsando ggdag_collider para identificar colliders en nuestro DAG.\n\n\n\n\nBueno, tenemos dos caminos, el que queremos estudiar y una puerta de atrás. Todo indica que tenemos que controlar por Diab y listo ¿No? Apliquemos esto agregando el parámetro adjust_for = \"Diab\" a la función ggdag_paths:\n\ndag_Ob %>% ggdag_paths(from = \"Ob\", to = \"Mort\",\n                       adjust_for = \"Diab\", shadow = TRUE) + \n  theme_dag() +\n  labs(hue = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nCaminos causales abiertos luego de controlar por Diab.\n\n\n\n\nSin embargo, podemos ver que los caminos abiertos son:\n\\[\n\\begin{array}\n_Ob  \\longrightarrow Mort\\\\\nOb  \\longrightarrow Diab \\longleftarrow Smok \\longrightarrow Mort\n\\end{array}\n\\] ¿Qué pasó? Bueno, lo que pasó es que al controlar por un collider abrimos un camino que estaba cerrado. Miremos qué pasa si controlamos por Diab y Smok:\n\ndag_Ob %>% ggdag_paths(from = \"Ob\", to = \"Mort\",\n                       adjust_for = c(\"Diab\", \"Smok\"), shadow = TRUE) + \n  theme_dag() +\n  labs(hue = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nCaminos causales abiertos luego de controlar por Diab y Smok.\n\n\n\n\nAhora sí, el único camino abierto es \\(Ob \\rightarrow Mort\\), que es la relaciòn causal que queremos estudiar. Finalmente, el modelo que deberíamos ajustar es:\n\\[\nMortalidad_i = \\alpha + \\beta_{Ob} Obesidad_i + \\beta_{Diab} Diabetes_i +  \\beta_{Smok} Smoker_i + \\epsilon_i\n\\]\nDonde \\(\\beta_{Ob}\\) es un estimador del efecto causal que queremos estudiar.\nPara más ejemplos y detalles sobre los DAGS pueden consultar (Cunningham 2021) o (Huntington-Klein 2021). El canal de YouTube de Nick Huntington-Klein también es un excelente recurso para profundizar sobre estos temas5.5 Nick es el autor de (Huntington-Klein 2021).\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. Chapman; Hall/CRC.\n\n\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale university press."
  },
  {
    "objectID": "exp_aleatorios.html",
    "href": "exp_aleatorios.html",
    "title": "5  Experimentos aleatorios",
    "section": "",
    "text": "En construcción 🚧"
  },
  {
    "objectID": "exp_aleatorios.html#sdf",
    "href": "exp_aleatorios.html#sdf",
    "title": "5  Experimentos aleatorios",
    "section": "5.1 Sdf",
    "text": "5.1 Sdf"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cunningham, Scott. 2021. Causal Inference: The Mixtape. Yale\nuniversity press.\n\n\nHerzog, Michael H, Gregory Francis, and Aaron Clarke. 2019.\nUnderstanding Statistics and Experimental Design: How to Not Lie\nwith Statistics. Springer Nature.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in\nStatistical Inference. Springer Science & Business Media.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "dags.html#causalidad-sin-correlación",
    "href": "dags.html#causalidad-sin-correlación",
    "title": "4  Grafos acíclicos dirigidos (DAGS)",
    "section": "4.3 Causalidad sin correlación",
    "text": "4.3 Causalidad sin correlación\nMuchas veces escuchamos que “correlación no implica causalidad” pero ¿Causalidad implica correlación? Respondamos esta pregunta con un ejemplo. Supongamos que hay una relación causal entre el ingreso (I) y la satisfacción (S), a más ingreso más satisfacción."
  },
  {
    "objectID": "intro_R.html#tidy-data",
    "href": "intro_R.html#tidy-data",
    "title": "1  R y el tidyverse",
    "section": "1.1 Tidy data",
    "text": "1.1 Tidy data\nLo primero que tenemos que pensar cuando trabajamos con el tidyverse es que nuestros datos estén en formato tidy. ¿Qué significa esto? Cuando un dataset está en formato tidy, cada columna corresponde a una variable y cada fila a una única observación2. Veamos un ejemplo. Tenemos tres sujetos a los cuales les medimos el tiempo de respuesta en una tarea. Cada sujeto realiza dos repeticiones de esta medición, el trial 1 y el trial 2. En la tabla Table 1.1 podemos ver las dos formas de organizar esta información.2 El caso contrario sería en el que una fila contiene varios mediciones para distintos niveles de una variable. Este formato se conoce como wide.\n\n\nTable 1.1: Ejemplo de tablas tidy y wide.\n\n\n\n\n(a) Tidy \n \n  \n    sujeto \n    trial \n    tiempo_respuesta \n  \n \n\n  \n    Jerry \n    1 \n    0.0807501 \n  \n  \n    Jerry \n    2 \n    0.8343330 \n  \n  \n    Elaine \n    1 \n    0.6007609 \n  \n  \n    Elaine \n    2 \n    0.1572084 \n  \n  \n    George \n    1 \n    0.0073994 \n  \n  \n    George \n    2 \n    0.4663935 \n  \n\n\n\n\n\n\n(b) Wide \n \n  \n    sujeto \n    trial_1 \n    trial_2 \n  \n \n\n  \n    Jerry \n    0.4977774 \n    0.7725215 \n  \n  \n    Elaine \n    0.2897672 \n    0.8746007 \n  \n  \n    George \n    0.7328820 \n    0.1749406 \n  \n\n\n\n\n\n\nA lo largo de este capítulo iremos viendo los beneficios de almacenar los datos en formato tidy. Por supuesto que estas ventajas tienen su precio, principalemente que las bases de datos crecen mucho en tamaño si tenemos muchas medidas repetidas con distintos valores de las variables."
  },
  {
    "objectID": "intro_R.html#introducción-al-tidyverse",
    "href": "intro_R.html#introducción-al-tidyverse",
    "title": "1  R y el tidyverse",
    "section": "1.2 Introducción al Tidyverse",
    "text": "1.2 Introducción al Tidyverse\nComo contamos más arriba, el tidyverse es una colección cerca de 25 paquetes, todos relacionados con la carga, manejo, modificación y visualización de datos. La idea de este libro no es profundizar en todas sus capacidades pero consideramos importante presentar algunas de las funciones que más vamos a utilizar a lo largo del libro. Estas son funciones para leer datos del paquete {readr}, los verbos de {dplyr} para manipularlos, las funciones de {tidyR} para acomodarlos y el poderosísimo {ggplot2} para visualizarlos.\n\n1.2.1 Cargando datos con readr\nUna de las cosas que vamos a hacer más a menudo en este libro es cargar algún dataset. Para esto vamos a usar varias de las funcionalidades del paquete {readr}.\nEl caso más simple al que nos vamos a enfrentar es la carga de una base de datos organizada en columnas y separadas por comas en un archivo de extensión .csv. En este caso lo que tenemos que hacer es bastante simple, usar la función read_csv() como a continuación:\n\nsummer <- read_csv(\"../data/summer.csv\")\n\nPodemos ver que al cargar los datos read_csv nos dice que hay ocho columnas chr (o sea de texto) y una dbl (o sea, un número). Si usamos la función summary podemos ver un detalle de cada avriable con su tipo y alguna descripción3:3 Existen alternativas para visualizar rápidamente un conjunto de datos como str o glimpse o la función skim del paquete {skimr}.\n\nsummary(summer)\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\nLos datos adentro de summer.csv son los ganadores de medallas en los juegos olímpicos de verano. Podemos ver algunas filas de muestra:\n\nhead(summer)\n#> # A tibble: 6 × 9\n#>    Year City   Sport    Discipline Athlete               Country Gender\n#>   <dbl> <chr>  <chr>    <chr>      <chr>                 <chr>   <chr> \n#> 1  1896 Athens Aquatics Swimming   HAJOS, Alfred         HUN     Men   \n#> 2  1896 Athens Aquatics Swimming   HERSCHMANN, Otto      AUT     Men   \n#> 3  1896 Athens Aquatics Swimming   DRIVAS, Dimitrios     GRE     Men   \n#> 4  1896 Athens Aquatics Swimming   MALOKINIS, Ioannis    GRE     Men   \n#> 5  1896 Athens Aquatics Swimming   CHASAPIS, Spiridon    GRE     Men   \n#> 6  1896 Athens Aquatics Swimming   CHOROPHAS, Efstathios GRE     Men   \n#> # ℹ 2 more variables: Event <chr>, Medal <chr>\n\nEl formato en el que read_csv almacena los datos se llama tibble y es el formato por excelencia del tidyverse. De momento lo único que nos importa es que es un formato que almacena los casos en filas y las variables en columnas (cada variable tiene un formato). Para más información sobre las cualidades de este formato, les recomiendo revisar la documentación.\n\n\n1.2.2 El operador pipe (|>) del paquete {magrittr}\nEl operador pipe nos permite concatenar funciones que utilizan como entrada los mismos datos. El principio de operación es el siguiente, supongan que nosotros queremos cargar un dataset y aplicarle la función summary. Esto lo podemos hacer simplemente cargando el dataset en una lìnea de código y ejecutanco la función summary() en la siguiente.\n\ndata <- read_csv(\"../data/summer.csv\")\nsummary(data)\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\nPero, también podemos aprovechar el operador pipe y hacer todo en una única línea de código.\n\nread_csv(\"../data/summer.csv\") |> summary()\n#>       Year          City              Sport            Discipline       \n#>  Min.   :1896   Length:31165       Length:31165       Length:31165      \n#>  1st Qu.:1948   Class :character   Class :character   Class :character  \n#>  Median :1980   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   :1970                                                           \n#>  3rd Qu.:2000                                                           \n#>  Max.   :2012                                                           \n#>    Athlete            Country             Gender             Event          \n#>  Length:31165       Length:31165       Length:31165       Length:31165      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>     Medal          \n#>  Length:31165      \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#> \n\nAl dejar vacío el paréntesis de la función summary(), la misma va a tomar como variable de entrada a la que está antes del operador pipe, es decir, a la que antes llamamos data. En el caso que la función summary() tuviera más de una variable de entrada, lo que viene antes del pipe tomaría el lugar de la primera de ellas.\nSi bien esta funcionalidad parece algo que complica las cosas y que no trae demasiados beneficios con un ejemplo tan simple, más adelante veremos que puede ser de gran utilidad, ayudando a disminuir la cantidad de línes de código y de variables intermedias.\n\n\n1.2.3 {dplyr} y sus verbos\nUna de las cosas más útiles del tidyverse para el tipo de procesamiento de datos que vamos a llevar a cabo en este libro son los verbos de dplyr. Estas funciones no permiten agregar columnas, resumir la información, filtrar filas, seleccionar columnas, etc4. Y todas estas acciones las podemos hacer en la base de datos completa o en una parte de ella agrupada de acuerdo a algún criterio. Vayamos de a poco.4 Para más detalles sobre los verbos disponibles en el paquete {dplyr} pueden visital este la página de referencia.\n\n1.2.3.1 El verbo filter\nVolvamos a los datos de los JJOO de verano. Supongamos que nos queremos quedar sólo con las medallas de Argentina. Para este tipo de filtrado de filas (o casos, o mediciones) {dplyr} tiene un verbo que se llama filter y funciona de la siguiente forma5:5 Se preguntarán por qué antes de la función filter aparece un ::dplyr. Esto es simplemente una forma de decirle a R que la función filter que debe utilizar es la del paquete {dplyr}. Esta es una práctica recomendable sobre todo para funciones con nombres comunes como filter o select.\n\nsummer |> dplyr::filter(Country == \"ARG\") |> head(10)\n#> # A tibble: 10 × 9\n#>    Year City  Sport     Discipline Athlete          Country Gender\n#>   <dbl> <chr> <chr>     <chr>      <chr>            <chr>   <chr> \n#> 1  1924 Paris Athletics Athletics  BRUNETO, Luis    ARG     Men   \n#> 2  1924 Paris Boxing    Boxing     PORZIO, Alfredo  ARG     Men   \n#> 3  1924 Paris Boxing    Boxing     QUARTUCCI, Pedro ARG     Men   \n#> 4  1924 Paris Boxing    Boxing     COPELLO, Alfredo ARG     Men   \n#> 5  1924 Paris Boxing    Boxing     MENDEZ, Hector   ARG     Men   \n#> 6  1924 Paris Polo      Polo       KENNY, Arturo    ARG     Men   \n#> # ℹ 4 more rows\n#> # ℹ 2 more variables: Event <chr>, Medal <chr>\n\nNoten que estamos utilizando el operador |> para concatenar las acciones: Con los datos de summer hacemos el filtrado y, luego, mostramos las primeras diez filas de esos datos ya filtrados.\nTambién podríamos quere quedarnos con las medallas de Argenitna en los JJOO de Atenas 2004, para esto debemos el operador lógico “y”, cuyo símbolo en R es &:\n\nsummer |> dplyr::filter(Country == \"ARG\" & Year == 2004) |> head(5)\n#> # A tibble: 5 × 9\n#>    Year City   Sport      Discipline Athlete                   Country Gender\n#>   <dbl> <chr>  <chr>      <chr>      <chr>                     <chr>   <chr> \n#> 1  2004 Athens Aquatics   Swimming   BARDACH, Georgina         ARG     Women \n#> 2  2004 Athens Basketball Basketball DELFINO, Carlos Francisco ARG     Men   \n#> 3  2004 Athens Basketball Basketball FERNANDEZ, Gabriel Diego  ARG     Men   \n#> 4  2004 Athens Basketball Basketball GINOBILI, Emanuel David   ARG     Men   \n#> 5  2004 Athens Basketball Basketball GUTIERREZ, Leonardo Mart… ARG     Men   \n#> # ℹ 2 more variables: Event <chr>, Medal <chr>\n\nQue linda esa Generación Dorada🏅, ¿No?.Por otro lado, si nos queremos quedar con las medallas de Argentina o Brasil debemos utilizar el operador lógico “o”, cuyo símbolo en R es |:\n\nsummer |> dplyr::filter(Country == \"ARG\" | Country == \"BRA\") |> head(10)\n#> # A tibble: 10 × 9\n#>    Year City    Sport    Discipline Athlete                   Country Gender\n#>   <dbl> <chr>   <chr>    <chr>      <chr>                     <chr>   <chr> \n#> 1  1920 Antwerp Shooting Shooting   PARAENSE, Guilherme       BRA     Men   \n#> 2  1920 Antwerp Shooting Shooting   BARBOSA, Dario            BRA     Men   \n#> 3  1920 Antwerp Shooting Shooting   DA COSTA, Afranio Antonio BRA     Men   \n#> 4  1920 Antwerp Shooting Shooting   PARAENSE, Guilherme       BRA     Men   \n#> 5  1920 Antwerp Shooting Shooting   SOLEDADE, Fernando        BRA     Men   \n#> 6  1920 Antwerp Shooting Shooting   WOLF, Sebastiao           BRA     Men   \n#> # ℹ 4 more rows\n#> # ℹ 2 more variables: Event <chr>, Medal <chr>\n\nAunque, una alternativa muy útil cuando tenemos los valores de una variable que queremos filtrar en un array es:\n\nsummer |> dplyr::filter(Country %in% c(\"ARG\", \"BRA\")) |> head(10)\n#> # A tibble: 10 × 9\n#>    Year City    Sport    Discipline Athlete                   Country Gender\n#>   <dbl> <chr>   <chr>    <chr>      <chr>                     <chr>   <chr> \n#> 1  1920 Antwerp Shooting Shooting   PARAENSE, Guilherme       BRA     Men   \n#> 2  1920 Antwerp Shooting Shooting   BARBOSA, Dario            BRA     Men   \n#> 3  1920 Antwerp Shooting Shooting   DA COSTA, Afranio Antonio BRA     Men   \n#> 4  1920 Antwerp Shooting Shooting   PARAENSE, Guilherme       BRA     Men   \n#> 5  1920 Antwerp Shooting Shooting   SOLEDADE, Fernando        BRA     Men   \n#> 6  1920 Antwerp Shooting Shooting   WOLF, Sebastiao           BRA     Men   \n#> # ℹ 4 more rows\n#> # ℹ 2 more variables: Event <chr>, Medal <chr>\n\nFinalmente, si tenemos una variable numérica, podemos filtrar con condiciones como mayor o menor:\n\nsummer |> dplyr::filter(Year > 2010) |> head(5)\n#> # A tibble: 5 × 9\n#>    Year City   Sport    Discipline Athlete          Country Gender\n#>   <dbl> <chr>  <chr>    <chr>      <chr>            <chr>   <chr> \n#> 1  2012 London Aquatics Diving     BOUDIA, David    USA     Men   \n#> 2  2012 London Aquatics Diving     QIU, Bo          CHN     Men   \n#> 3  2012 London Aquatics Diving     DALEY, Thomas    GBR     Men   \n#> 4  2012 London Aquatics Diving     CHEN, Ruolin     CHN     Women \n#> 5  2012 London Aquatics Diving     BROBEN, Brittany AUS     Women \n#> # ℹ 2 more variables: Event <chr>, Medal <chr>\n\n\n\n1.2.3.2 El verbo select\nEl verbo select es similar a filter pero nos permite filtrar no casos sino variables. Por ejemplo, ¿Qué pasa si solo nos interesa el año, la ciudad y el nombre del atleta?:\n\nsummer |> dplyr::select(c(Year, City, Athlete)) |> head(5)\n#> # A tibble: 5 × 3\n#>    Year City   Athlete           \n#>   <dbl> <chr>  <chr>             \n#> 1  1896 Athens HAJOS, Alfred     \n#> 2  1896 Athens HERSCHMANN, Otto  \n#> 3  1896 Athens DRIVAS, Dimitrios \n#> 4  1896 Athens MALOKINIS, Ioannis\n#> 5  1896 Athens CHASAPIS, Spiridon\n\n\n\n1.2.3.3 El verbo mutate\nAhora las cosas se complican un poco. mutate es un verbo que nos permite crear nuevas columnas ya sea con datos nuevos o en función de los datos existentes. Por ejemplo, creemos una columna nueva que tenga un chr con el país, un guión y el nombre del atleta y llamémosla nationality_athlete. Nos vamos a quedar sólo con el año, la medalla que ganó y el nuevo nombre combinado con la nacionalidad:\n\nsummer |> \n  dplyr::mutate(nationality_athlete = paste(Country, \"-\", Athlete)) |> \n  dplyr::select(c(Year, Medal, nationality_athlete)) |>\n  head(5)\n#> # A tibble: 5 × 3\n#>    Year Medal  nationality_athlete     \n#>   <dbl> <chr>  <chr>                   \n#> 1  1896 Gold   HUN - HAJOS, Alfred     \n#> 2  1896 Silver AUT - HERSCHMANN, Otto  \n#> 3  1896 Bronze GRE - DRIVAS, Dimitrios \n#> 4  1896 Gold   GRE - MALOKINIS, Ioannis\n#> 5  1896 Silver GRE - CHASAPIS, Spiridon\n\nO, por ejemplo, podemos querer crear una variable que nos ponga un \\(1\\) si es griego y un \\(0\\) si no6:6 Para más detalles sobre la función if_else pueden ver el siguiente link.\n\nsummer |> \n  dplyr::mutate(is_greek = if_else(Country == \"GRE\", 1, 0)) |> \n  dplyr::select(c(Year, Medal, Country, is_greek)) |>\n  head(5)\n#> # A tibble: 5 × 4\n#>    Year Medal  Country is_greek\n#>   <dbl> <chr>  <chr>      <dbl>\n#> 1  1896 Gold   HUN            0\n#> 2  1896 Silver AUT            0\n#> 3  1896 Bronze GRE            1\n#> 4  1896 Gold   GRE            1\n#> 5  1896 Silver GRE            1\n\nAhora vamos a aprender algo muy importante y cool 🆒: A agrupar los casos de acuerdo a una variable. Por ejemplo, si queremos agregar una columna que contenga la cantidad total de medallas ganadas por un país a cada atleta de ese país podemos hacer lo siguiente:\n\nsummer |> \n  group_by(Country) |>\n  dplyr::mutate(num_medals = n()) |> \n  dplyr::select(c(Year, Medal, Athlete, num_medals)) |>\n  head(5)\n#> # A tibble: 5 × 5\n#> # Groups:   Country [3]\n#>   Country  Year Medal  Athlete            num_medals\n#>   <chr>   <dbl> <chr>  <chr>                   <int>\n#> 1 HUN      1896 Gold   HAJOS, Alfred            1079\n#> 2 AUT      1896 Silver HERSCHMANN, Otto          146\n#> 3 GRE      1896 Bronze DRIVAS, Dimitrios         148\n#> 4 GRE      1896 Gold   MALOKINIS, Ioannis        148\n#> 5 GRE      1896 Silver CHASAPIS, Spiridon        148\n\n¿Perdidos? Tomensé su tiepo para tratar de entender qué pasó y prueben distintas alternativas en sus computadoras.\n\n\n1.2.3.4 El verbo summarise\nPor último, el verbo summarise nos permite sacar medidas resumen de nuestros datos. Empecemos con algo obvio: ¿Cuántas medallas de oro ganó cada país en la historia de los juegos olímpicos?. Podemos hacer algo parecido a lo último que hicimos con mutate pero el resultados será ligeramente diferente7:7 La función arrange nos ordena los datos de acuerdo a la variable que le enviemos como parámetro de menos a mayor. Si queremos que ordene de mayor a menor debemos agregar la función desc en el argumento. Más detalles acá.\n\nsummer |> \n  dplyr::filter(Medal == \"Gold\") |>\n  group_by(Country) |>\n  dplyr::summarise(num_medals = n()) |>\n  arrange(desc(num_medals)) |>\n  head(10)\n#> # A tibble: 10 × 2\n#>   Country num_medals\n#>   <chr>        <int>\n#> 1 USA           2235\n#> 2 URS            838\n#> 3 GBR            546\n#> 4 ITA            476\n#> 5 GER            452\n#> 6 HUN            412\n#> # ℹ 4 more rows\n\nHay algo raro, ¿No? Bueno, sí, de esta forma estamos contando a todos los atletas que tuvieron la misma medalla (por ejemplo, si la medalla fue por fútbol estamos contando cerca de 30 medallas). Para resolver esto nos podemos sacar de encima los casos duplicados por año, deporte, disciplina, evento y género8:8 La función distinct nos conserva una sola realización de cada caso que es igual de acuerdo a las variables que le pasemos como parámetros. Más detalles acá.\n\nsummer |> \n  distinct(Year, Sport, Discipline, Event, Gender, .keep_all = TRUE) |>\n  dplyr::filter(Medal == \"Gold\") |>\n  group_by(Country) |>\n  dplyr::summarise(num_medals = n()) |>\n  arrange(desc(num_medals)) |>\n  head(5)\n#> # A tibble: 5 × 2\n#>   Country num_medals\n#>   <chr>        <int>\n#> 1 USA             67\n#> 2 GBR             46\n#> 3 CHN             40\n#> 4 RUS             22\n#> 5 GER             19\n\nVayamos con lo último, calculemos la media y la desviación estándar de las medallas de Argentina por JJOO combinando todo lo que vimos.\n\nsummer |> \n  distinct(Year, Sport, Discipline, Event, Medal, Gender, .keep_all = TRUE) |>\n  dplyr::filter(Country == \"ARG\") |>\n  group_by(Country, Year) |>\n  dplyr::summarise(num_medals = n()) |>\n  ungroup() |>\n  summarise(media  = mean(num_medals),\n            desvio = sd(num_medals))\n#> # A tibble: 1 × 2\n#>   media desvio\n#>   <dbl>  <dbl>\n#> 1  3.83   2.28\n\nDigieran esto tranquilos.\n\n\n\n1.2.4 {tidyR}, el paquete para ordenar tus datos\nEl paquete {tidyR} tiene muchas herramientas de manejo de tablas como reformatear, expandir tablas, manejar valores faltantes, dividir celdas, anidar datos, etc9. Sin embargo, en esta breve introducción sólo vamos a presentar muy brevemente las herramientas que nos permiten convertir una tabla wide en tidy (o long) y viceverse.9 Para más información ver el cheatsheet.\n\n1.2.4.1 La función pivot_longer\nVolvamos a la tabla iniicial que teníamos en formato wide:\n\ntabla_wide <- tibble(sujeto  = rep(c(\"Jerry\", \"Elaine\", \"George\")),\n                     trial_1 = runif(3),\n                     trial_2 = runif(3)) \n\ntabla_wide\n#> # A tibble: 3 × 3\n#>   sujeto trial_1 trial_2\n#>   <chr>    <dbl>   <dbl>\n#> 1 Jerry    0.320  0.404 \n#> 2 Elaine   0.402  0.0637\n#> 3 George   0.196  0.389\n\nSi nosotros quisiñeramos transformar esta tabla en una tabla en formato tidy podemos utilizar la función pivot_longer10. Veamos como funciona y después la desmenuzamos:10 Más información acá.\n\npivot_longer(data = tabla_wide, \n             cols = trial_1:trial_2, \n             names_to = \"trial\",\n             values_to = \"tiempo_respuesta\")\n#> # A tibble: 6 × 3\n#>   sujeto trial   tiempo_respuesta\n#>   <chr>  <chr>              <dbl>\n#> 1 Jerry  trial_1           0.320 \n#> 2 Jerry  trial_2           0.404 \n#> 3 Elaine trial_1           0.402 \n#> 4 Elaine trial_2           0.0637\n#> 5 George trial_1           0.196 \n#> 6 George trial_2           0.389\n\nLos argumentos son los siguientes: data es la tabla a la que le vamos a realizar el cambio de formato; cols son las columnas que vamos a cambiar, en este caso desde trial_1 a trial_2; en names_to indicamos la variable a la que vamos a mandar los nombres de las columnas actuales; y values_to la variables a la que vamos a mandar los valores.\nAlgo ligeramente raro es que la columna trial no es numérica y, sólo por completitud, lo vamos a solucionar usando a nuestro gran amigo |> y al verbo mutate11:11 Y la función parse_number del paquete {readr}.\n\npivot_longer(data = tabla_wide, \n             cols = trial_1:trial_2, \n             names_to = \"trial\",\n             values_to = \"tiempo_respuesta\") |>\n  mutate(trial = parse_number(trial))\n#> # A tibble: 6 × 3\n#>   sujeto trial tiempo_respuesta\n#>   <chr>  <dbl>            <dbl>\n#> 1 Jerry      1           0.320 \n#> 2 Jerry      2           0.404 \n#> 3 Elaine     1           0.402 \n#> 4 Elaine     2           0.0637\n#> 5 George     1           0.196 \n#> 6 George     2           0.389\n\n\n\n1.2.4.2 La función pivot_wider\nAhora vamos con el caso contrario en el que tenemos una tabla en formato long y la queremos convertir en wide:\n\ntabla_long <- pivot_longer(data = tabla_wide, \n                           cols = trial_1:trial_2, \n                           names_to = \"trial\",\n                           values_to = \"tiempo_respuesta\") |>\n  mutate(trial = parse_number(trial))\n\ntabla_long\n#> # A tibble: 6 × 3\n#>   sujeto trial tiempo_respuesta\n#>   <chr>  <dbl>            <dbl>\n#> 1 Jerry      1           0.320 \n#> 2 Jerry      2           0.404 \n#> 3 Elaine     1           0.402 \n#> 4 Elaine     2           0.0637\n#> 5 George     1           0.196 \n#> 6 George     2           0.389\n\nPara esto vamos a hechar mano a la función pivot_wider12 que tiene una sintáxis parecida a su prima pivot_longer:12 Más información acá.\n\npivot_wider(data = tabla_long, \n            names_from = trial, \n            values_from = tiempo_respuesta)\n#> # A tibble: 3 × 3\n#>   sujeto   `1`    `2`\n#>   <chr>  <dbl>  <dbl>\n#> 1 Jerry  0.320 0.404 \n#> 2 Elaine 0.402 0.0637\n#> 3 George 0.196 0.389\n\nEt Voilà!, ya tenemos nuestra tabla en formato wide. En este caso le dijimos de que variable tomar los nombres de las nuevas columnas en names_from y de que variable tomar los valores en values_from.\nFinalmente, y sólo para alimnetar nuestra obsesión, vamos a corregir los nombres de las columnas agregando el prefijo trial_ utilizando el parámetro de la función names_prefix:\n\npivot_wider(data = tabla_long, \n            names_from = trial, \n            names_prefix = \"trial_\",\n            values_from = tiempo_respuesta)\n#> # A tibble: 3 × 3\n#>   sujeto trial_1 trial_2\n#>   <chr>    <dbl>   <dbl>\n#> 1 Jerry    0.320  0.404 \n#> 2 Elaine   0.402  0.0637\n#> 3 George   0.196  0.389"
  },
  {
    "objectID": "intro_R.html#algunas-palabras-finales",
    "href": "intro_R.html#algunas-palabras-finales",
    "title": "1  R y el tidyverse",
    "section": "1.3 Algunas palabras finales",
    "text": "1.3 Algunas palabras finales\nComo vimos brevemente en este capítulo, los paquetes del tidyverse son una herramineta importantísima para el análisis de datos utilizando R. Para más detalles sobre estas funcionalidades les recomendamos la guía de Hadley Wickham(Wickham et al. 2019) o, si ya se quieren sumergir de lleno en el mundo del análisis de datos con R, este fantástico libro [Wickham, Çetinkaya-Rundel, and Grolemund (2023)]13. Es decir, sin tener que cargar ningún paquete de funciones adicional..13 Disponible gratis online en acá.\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686.\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  }
]